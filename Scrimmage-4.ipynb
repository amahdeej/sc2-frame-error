{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import pathlib\n",
    "import sqlite3\n",
    "import tempfile\n",
    "import importlib\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from configparser import ConfigParser\n",
    "from distutils.spawn import find_executable\n",
    "from collections import OrderedDict, namedtuple\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sru import SRU, SRUCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2' # GPU Number\n",
    "os.environ['TORCH_EXTENSIONS_DIR'] = os.popen('pwd').read().strip('\\n') # SRU Temp Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and Standard Deviation for Noise - used for SNR analysis\n",
    "scr4_mean = 31.827541\n",
    "scr4_std = 7.5468507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10 # No Of Training Epochs\n",
    "TRAIN_BATCH_SIZE = 1024 # Batch size for Training\n",
    "VAL_BATCH_SIZE = 128 # Batch size for Validation and Testing\n",
    "\n",
    "# Parameters for RNN based Architectures\n",
    "EMBEDDING_DIM = 2\n",
    "HIDDEN_DIM = 100\n",
    "TAGSET_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(2, 50)\n",
    "        self.selu = nn.SELU()\n",
    "        self.linear2 = nn.Linear(50, 50)\n",
    "        self.selu = nn.SELU()\n",
    "        self.linear3 = nn.Linear(50, 50)\n",
    "        self.selu = nn.SELU()        \n",
    "        self.drop1 = nn.AlphaDropout(p=0.5)\n",
    "        self.linear5 = nn.Linear(50,2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.linear1(input)\n",
    "        output = self.selu(output)\n",
    "        output = self.drop1(output)\n",
    "        output = self.linear2(output)\n",
    "        output = self.selu(output)\n",
    "        output = self.drop1(output)\n",
    "        output = self.linear3(output)\n",
    "        output = self.selu(output)\n",
    "        output = self.drop1(output)\n",
    "        output = self.linear5(output)\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
    "        super(GRUTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim)        \n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "            \n",
    "    def forward(self, sentence):\n",
    "        gru_out, _ = self.gru(sentence.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(gru_out.view(len(sentence), -1))\n",
    "        return tag_space\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BGRUTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
    "        super(BGRUTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bgru = nn.GRU(embedding_dim, hidden_dim,bidirectional=True)        \n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "            \n",
    "    def forward(self, sentence):\n",
    "        gru_out, _ = self.bgru(sentence.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(gru_out.view(len(sentence), -1))\n",
    "        return tag_space\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSRUTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
    "        super(BSRUTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bsru = SRU(embedding_dim, hidden_dim,2,bidirectional=True)        \n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        sru_out, _ = self.bsru(sentence.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(sru_out.view(len(sentence), -1))\n",
    "        return tag_space\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBSDNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        nn.init.xavier_normal_(self.conv1.weight)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
    "        nn.init.xavier_normal_(self.conv2.weight)\n",
    "\n",
    "        self.linear1 = nn.Linear(16*2, 100)\n",
    "        \n",
    "        self.bsru = SRU(input_size=100, hidden_size=100,num_layers=2,bidirectional=True)\n",
    "        self.linear2 = nn.Linear(100*2, 100*2)\n",
    "        nn.init.xavier_normal_(self.linear2.weight)\n",
    "\n",
    "        self.linear3 = nn.Linear(100*2, 2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        output = self.conv1(input)\n",
    "        output = self.relu(output)\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout1(output)\n",
    "                \n",
    "        output = output.view(-1, 16*2)\n",
    "        output1 = self.linear1(output)\n",
    "\n",
    "        output = output1.view(len(output1),1 ,-1 )\n",
    "        output, _ = self.bsru(output)\n",
    "        \n",
    "        output = output.view(len(output1), -1)\n",
    "\n",
    "        output = self.linear2(output)\n",
    "        output = self.relu(output)\n",
    "\n",
    "        output = self.dropout2(output)\n",
    "\n",
    "        output = self.linear3(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scrimmage4_link_dataset.pickle', 'rb') as file:\n",
    "    link_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain only the features selected by the RFE method\n",
    "# Columns : 0 - SNR, 1 - MCS, 2 - Center Frequency, 3 - Bandwidth, [4:19] - PSD\n",
    "cols = torch.LongTensor([0,1]) \n",
    "link_data = [(link_datas[0][:,cols], link_datas[1]) for link_datas in link_dataset] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Pilot\n",
    "test_start = 142\n",
    "total_links = len(link_dataset)\n",
    "# Do a 40:10:50 train:validation:test split \n",
    "train_data = link_data[:114]\n",
    "val_data = link_data[114:142]\n",
    "test_data = link_data[142:]\n",
    "\n",
    "train_x = torch.cat(tuple(link[0] for link in train_data),dim=0)\n",
    "train_y = torch.cat(tuple(link[1] for link in train_data),dim=0)\n",
    "\n",
    "val_x = torch.cat(tuple(link[0] for link in val_data),dim=0)\n",
    "val_y = torch.cat(tuple(link[1] for link in val_data),dim=0)\n",
    "\n",
    "test_x = torch.cat(tuple(link[0] for link in test_data),dim=0)\n",
    "test_y = torch.cat(tuple(link[1] for link in test_data),dim=0)\n",
    "\n",
    "# Reshape data for CBSDNN\n",
    "train_x_cbsdnn = train_x.view(-1,1,2)\n",
    "val_x_cbsdnn = val_x.view(-1,1,2)\n",
    "test_x_cbsdnn = test_x.view(-1,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Pilot\n",
    "\n",
    "datadist = [0.4, 0.1, 0.5] # distribution among train, test and validation\n",
    "\n",
    "train_x_npn = []\n",
    "train_y_npn = []\n",
    "val_x_npn = []\n",
    "val_y_npn = []\n",
    "test_x_npn = []\n",
    "test_y_npn = []\n",
    "\n",
    "for i in range(len(link_data)):\n",
    "    datalen = len(link_data[i][1])\n",
    "    trainlen = int(datalen*datadist[0])\n",
    "    vallen = int(datalen*(datadist[0]+datadist[1]))\n",
    "\n",
    "    train_x_npn.append(link_data[i][0][0:trainlen].numpy())    \n",
    "    train_y_npn.append(link_data[i][1][0:trainlen].numpy())\n",
    "    \n",
    "    val_x_npn.append(link_data[i][0][trainlen:vallen].numpy())    \n",
    "    val_y_npn.append(link_data[i][1][trainlen:vallen].numpy())\n",
    "    \n",
    "    test_x_npn.append(link_data[i][0][vallen:datalen].numpy())\n",
    "    test_y_npn.append(link_data[i][1][vallen:datalen].numpy())\n",
    "\n",
    "train_x_np = np.concatenate(train_x_npn)\n",
    "train_y_np = np.concatenate(train_y_npn)\n",
    "val_x_np = np.concatenate(val_x_npn)\n",
    "val_y_np = np.concatenate(val_y_npn)\n",
    "test_x_np = np.concatenate(test_x_npn)\n",
    "test_y_np = np.concatenate(test_y_npn)\n",
    "\n",
    "train_x_plt = torch.from_numpy(train_x_np)\n",
    "val_x_plt = torch.from_numpy(val_x_np)\n",
    "test_x_plt = torch.from_numpy(test_x_np)\n",
    "train_y_plt = torch.from_numpy(train_y_np)\n",
    "val_y_plt = torch.from_numpy(val_y_np)\n",
    "test_y_plt = torch.from_numpy(test_y_np)\n",
    "\n",
    "# Reshape data for CBSDNN\n",
    "train_x_plt_cbsdnn = train_x_plt.view(-1,1,2)\n",
    "val_x_plt_cbsdnn = val_x_plt.view(-1,1,2)\n",
    "test_x_plt_ccbsnn = test_x_plt.view(-1,1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Result Variables\n",
    "mlp_scr4_res = {}\n",
    "gru_scr4_res = {}\n",
    "bgru_scr4_res = {}\n",
    "bsru_scr4_res = {}\n",
    "cbsdnn_scr4_res = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Pilot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Validation:   0%|                                  | 0/4757 [00:23<?, ?it/s]\n",
      "Epoch 1/10, Training: 100%|████████████████████████| 2468/2468 [00:08<00:00, 295.82it/s]\n",
      "Epoch 1/10, Validation: 100%|██████████████████████| 4757/4757 [00:14<00:00, 323.24it/s]\n",
      "Epoch 2/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.63080, validation loss: 0.58914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Training: 100%|████████████████████████| 2468/2468 [00:08<00:00, 292.14it/s]\n",
      "Epoch 2/10, Validation: 100%|██████████████████████| 4757/4757 [00:15<00:00, 311.76it/s]\n",
      "Epoch 3/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.55983, validation loss: 0.54588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Training: 100%|████████████████████████| 2468/2468 [00:08<00:00, 289.82it/s]\n",
      "Epoch 3/10, Validation: 100%|██████████████████████| 4757/4757 [00:14<00:00, 321.60it/s]\n",
      "Epoch 4/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.55477, validation loss: 0.54453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Training: 100%|████████████████████████| 2468/2468 [00:08<00:00, 308.30it/s]\n",
      "Epoch 4/10, Validation: 100%|██████████████████████| 4757/4757 [00:15<00:00, 312.55it/s]\n",
      "Epoch 5/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.55293, validation loss: 0.54426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Training: 100%|████████████████████████| 2468/2468 [00:07<00:00, 313.37it/s]\n",
      "Epoch 5/10, Validation: 100%|██████████████████████| 4757/4757 [00:14<00:00, 328.96it/s]\n",
      "Epoch 6/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.55206, validation loss: 0.54547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Training: 100%|████████████████████████| 2468/2468 [00:08<00:00, 303.09it/s]\n",
      "Epoch 6/10, Validation: 100%|██████████████████████| 4757/4757 [00:15<00:00, 313.49it/s]\n",
      "Epoch 7/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.55150, validation loss: 0.54407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Training: 100%|████████████████████████| 2468/2468 [00:07<00:00, 312.85it/s]\n",
      "Epoch 7/10, Validation: 100%|██████████████████████| 4757/4757 [00:14<00:00, 325.67it/s]\n",
      "Epoch 8/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.55107, validation loss: 0.54636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Training: 100%|████████████████████████| 2468/2468 [00:09<00:00, 259.79it/s]\n",
      "Epoch 8/10, Validation: 100%|██████████████████████| 4757/4757 [00:15<00:00, 299.04it/s]\n",
      "Epoch 9/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.55072, validation loss: 0.54343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Training: 100%|████████████████████████| 2468/2468 [00:08<00:00, 286.72it/s]\n",
      "Epoch 9/10, Validation: 100%|██████████████████████| 4757/4757 [00:15<00:00, 310.78it/s]\n",
      "Epoch 10/10, Validation:   0%|                                 | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.55040, validation loss: 0.54366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Training: 100%|███████████████████████| 2468/2468 [00:08<00:00, 295.93it/s]\n",
      "Epoch 10/10, Validation: 100%|█████████████████████| 4757/4757 [00:15<00:00, 315.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.55010, validation loss: 0.54104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "model = MLP().cuda()\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor([1.0,1.0])).cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "train_dataloader=data.DataLoader(data.TensorDataset(train_x,train_y),batch_size=TRAIN_BATCH_SIZE, \n",
    "                                   shuffle=True, num_workers=16, pin_memory=True)\n",
    "\n",
    "val_dataloader=data.DataLoader(data.TensorDataset(val_x,val_y),batch_size=VAL_BATCH_SIZE, \n",
    "                                 shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "test_dataloader=data.DataLoader(data.TensorDataset(test_x,test_y), batch_size=VAL_BATCH_SIZE,\n",
    "                                  shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "model.train()\n",
    "for epoch_idx in range(NUM_EPOCHS):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "\n",
    "    progress_training_epoch = tqdm(\n",
    "        train_dataloader, \n",
    "        desc=f'Epoch {epoch_idx + 1}/{NUM_EPOCHS}, Training',\n",
    "        miniters=1, ncols=88, position=0,\n",
    "        leave=True, total=len(train_dataloader), smoothing=.9)\n",
    "\n",
    "    progress_validation_epoch = tqdm(\n",
    "        val_dataloader, \n",
    "        desc=f'Epoch {epoch_idx + 1}/{NUM_EPOCHS}, Validation',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(val_dataloader), smoothing=.9)\n",
    "\n",
    "    train_loss = 0\n",
    "    train_size = 0\n",
    "    for idx, (input, target) in enumerate(progress_training_epoch):\n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "        model.zero_grad()\n",
    "        predict = model(input)\n",
    "        loss = loss_function(predict, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss * target.size()[0]\n",
    "        train_size += target.size()[0]\n",
    "\n",
    "    test_loss = 0\n",
    "    test_size = 0\n",
    "    predict = []\n",
    "    target = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (batch_input, batch_target) in enumerate(progress_validation_epoch):\n",
    "            batch_input = batch_input.cuda()\n",
    "            batch_target = batch_target.cuda()\n",
    "            batch_predict = model(batch_input)\n",
    "            loss = loss_function(batch_predict, batch_target)\n",
    "            predict.append(batch_predict.argmax(dim=1).cpu().numpy())\n",
    "            target.append(batch_target.cpu().numpy())        \n",
    "            test_loss += loss * batch_target.size()[0]\n",
    "            test_size += batch_target.size()[0]\n",
    "    predict = np.concatenate(predict, axis=0)\n",
    "    target = np.concatenate(target, axis=0)\n",
    "   \n",
    "    print(f'train loss:{train_loss.item()/train_size: .5f}, '\n",
    "          f'validation loss:{test_loss.item()/test_size: .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test: 100%|█████████████████████████| 26457/26457 [00:33<00:00, 796.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "progress_test_epoch = tqdm(\n",
    "    test_dataloader, \n",
    "    desc=f'Epoch {epoch_idx + 1}/{NUM_EPOCHS}, Test',\n",
    "    miniters=1, ncols=88, position=0, \n",
    "    leave=True, total=len(test_dataloader), smoothing=.9)\n",
    "\n",
    "predict = [] # Predicted Label\n",
    "target = [] # Target Lable\n",
    "snrss = [] # SNR (Calculated from Noise Variance)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (batch_input, batch_target) in enumerate(progress_test_epoch):\n",
    "        batch_input = batch_input.cuda()\n",
    "        batch_target = batch_target.cuda()\n",
    "        batch_predict = model(batch_input)\n",
    "        predict.append(batch_predict.argmax(dim=1).cpu().numpy())\n",
    "        target.append(batch_target.cpu().numpy())        \n",
    "        snrss.append(batch_input.cpu().numpy()[:,0])\n",
    "predict = np.concatenate(predict, axis=0)\n",
    "target = np.concatenate(target, axis=0)\n",
    "snrss = np.concatenate(snrss, axis=0)\n",
    "\n",
    "mlp_scr4_res['mlp_scr4_prd'] = predict\n",
    "mlp_scr4_res['mlp_scr4_trg'] = target\n",
    "mlp_scr4_res['mlp_scr4_snr'] = snrss * scr4_std + scr4_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test Accuracy per Link\n",
    "\n",
    "accr = []\n",
    "for i in range(total_links - test_start):\n",
    "    test_data_i = [link_data[i+test_start]]\n",
    "    test_x_i = torch.cat(tuple(link[0] for link in test_data_i),dim=0)\n",
    "    test_y_i = torch.cat(tuple(link[1] for link in test_data_i),dim=0)\n",
    "    test_x_i = test_x_i.view(-1,1,2)\n",
    "    \n",
    "    test_dataloader = data.DataLoader(\n",
    "    data.TensorDataset(test_x_i,test_y_i), \n",
    "    batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "    progress_test = tqdm(\n",
    "        test_dataloader, \n",
    "        desc=f'Link {i+test_start+1}/{total_links}, Test',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(test_dataloader), smoothing=.9)\n",
    "\n",
    "    predict = []\n",
    "    target = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (batch_input, batch_target) in enumerate(progress_test):\n",
    "            batch_input = batch_input.cuda()\n",
    "            batch_target = batch_target.cuda()\n",
    "            batch_predict = model(batch_input)\n",
    "            predict.append(batch_predict.argmax(dim=1).cpu().numpy())\n",
    "            target.append(batch_target.cpu().numpy())        \n",
    "    predict = np.concatenate(predict, axis=0)\n",
    "    target = np.concatenate(target, axis=0)\n",
    "\n",
    "    tp = predict[target==1].sum()\n",
    "    tn = (target==0).sum() - predict[target==0].sum()\n",
    "    fp = predict[target==0].sum()\n",
    "    fn = (target==1).sum() - predict[target==1].sum()\n",
    "    accr.append((tp+tn)/(tp+tn+fp+fn))    \n",
    "\n",
    "mlp_scr4_res['mlp_scr4_accr_perlink'] = accr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training: 100%|█████████████████████████| 2468/2468 [00:59<00:00, 41.43it/s]\n",
      "Epoch 1/10, Validation: 100%|███████████████████████| 4757/4757 [01:10<00:00, 67.02it/s]\n",
      "Epoch 2/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.54342, validation loss: 0.52424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Training: 100%|█████████████████████████| 2468/2468 [00:59<00:00, 41.20it/s]\n",
      "Epoch 2/10, Validation: 100%|███████████████████████| 4757/4757 [01:10<00:00, 67.07it/s]\n",
      "Epoch 3/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.52892, validation loss: 0.51896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Training: 100%|█████████████████████████| 2468/2468 [00:59<00:00, 41.48it/s]\n",
      "Epoch 3/10, Validation: 100%|███████████████████████| 4757/4757 [01:10<00:00, 67.27it/s]\n",
      "Epoch 4/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.52714, validation loss: 0.51846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Training: 100%|█████████████████████████| 2468/2468 [01:01<00:00, 40.39it/s]\n",
      "Epoch 4/10, Validation: 100%|███████████████████████| 4757/4757 [01:12<00:00, 65.84it/s]\n",
      "Epoch 5/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.52507, validation loss: 0.51846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Training: 100%|█████████████████████████| 2468/2468 [00:59<00:00, 41.31it/s]\n",
      "Epoch 5/10, Validation: 100%|███████████████████████| 4757/4757 [01:11<00:00, 66.62it/s]\n",
      "Epoch 6/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.52294, validation loss: 0.51821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Training: 100%|█████████████████████████| 2468/2468 [01:01<00:00, 40.03it/s]\n",
      "Epoch 6/10, Validation: 100%|███████████████████████| 4757/4757 [01:12<00:00, 65.37it/s]\n",
      "Epoch 7/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.52097, validation loss: 0.51729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Training: 100%|█████████████████████████| 2468/2468 [00:58<00:00, 42.15it/s]\n",
      "Epoch 7/10, Validation: 100%|███████████████████████| 4757/4757 [01:09<00:00, 68.24it/s]\n",
      "Epoch 8/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.51887, validation loss: 0.51563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Training: 100%|█████████████████████████| 2468/2468 [01:00<00:00, 41.07it/s]\n",
      "Epoch 8/10, Validation: 100%|███████████████████████| 4757/4757 [01:11<00:00, 66.92it/s]\n",
      "Epoch 9/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.51658, validation loss: 0.51290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Training: 100%|█████████████████████████| 2468/2468 [01:00<00:00, 41.06it/s]\n",
      "Epoch 9/10, Validation: 100%|███████████████████████| 4757/4757 [01:11<00:00, 66.64it/s]\n",
      "Epoch 10/10, Validation:   0%|                                 | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.51418, validation loss: 0.50905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Training: 100%|████████████████████████| 2468/2468 [00:59<00:00, 41.40it/s]\n",
      "Epoch 10/10, Validation: 100%|██████████████████████| 4757/4757 [01:10<00:00, 67.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.51165, validation loss: 0.50444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "model = GRUTagger(EMBEDDING_DIM, HIDDEN_DIM, TAGSET_SIZE).cuda()\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor([1.0,1.0])).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), 0.0001)\n",
    "\n",
    "train_dataloader=data.DataLoader(data.TensorDataset(train_x,train_y),batch_size=TRAIN_BATCH_SIZE, \n",
    "                                   shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "val_dataloader=data.DataLoader(data.TensorDataset(val_x,val_y),batch_size=VAL_BATCH_SIZE, \n",
    "                                 shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "test_dataloader=data.DataLoader(data.TensorDataset(test_x,test_y), batch_size=VAL_BATCH_SIZE,\n",
    "                                  shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "for epoch_idx in range(NUM_EPOCHS): \n",
    "\n",
    "    progress_training_epoch = tqdm(\n",
    "        train_dataloader, \n",
    "        desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Training',w\n",
    "        miniters=1, ncols=88, position=0,\n",
    "        leave=True, total=len(train_dataloader), smoothing=.9)\n",
    "\n",
    "    progress_validation_epoch = tqdm(\n",
    "        val_dataloader, \n",
    "        desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Validation',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(val_dataloader), smoothing=.9)\n",
    "\n",
    "    train_loss = 0\n",
    "    train_size = 0\n",
    "    model.train()\n",
    "    for idx, (sentence, tags) in enumerate(progress_training_epoch):\n",
    "        sentence = sentence.cuda()\n",
    "        tags = tags.cuda()\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(sentence)\n",
    "        loss = loss_function(tag_scores, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss * tags.size()[0]\n",
    "        train_size += tags.size()[0]\n",
    "\n",
    "    test_loss = 0\n",
    "    test_size = 0\n",
    "    predict = []\n",
    "    target = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (sentence, tags) in enumerate(progress_validation_epoch):\n",
    "            sentence = sentence.cuda()\n",
    "            tags = tags.cuda()\n",
    "            tag_scores = model(sentence)\n",
    "            loss = loss_function(tag_scores, tags)\n",
    "            predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "            target.append(tags.cpu().numpy())        \n",
    "            test_loss += loss * tags.size()[0]\n",
    "            test_size += tags.size()[0]\n",
    "    predict = np.concatenate(predict, axis=0)\n",
    "    target = np.concatenate(target, axis=0)\n",
    "\n",
    "    print(f'train loss:{train_loss.item()/train_size: .5f}, '\n",
    "          f'validation loss:{test_loss.item()/test_size: .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test: 100%|█████████████████████████| 26457/26457 [00:56<00:00, 471.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "progress_test_epoch = tqdm(\n",
    "    test_dataloader, \n",
    "    desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Test',\n",
    "    miniters=1, ncols=88, position=0, \n",
    "    leave=True, total=len(test_dataloader), smoothing=.9)\n",
    "\n",
    "predict = []\n",
    "target = []\n",
    "snrss = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (sentence, tags) in enumerate(progress_test_epoch):\n",
    "        sentence = sentence.cuda()\n",
    "        tags = tags.cuda()\n",
    "        tag_scores = model(sentence)\n",
    "        predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "        target.append(tags.cpu().numpy())\n",
    "        snrss.append(sentence.cpu().numpy()[:,0])\n",
    "        \n",
    "predict = np.concatenate(predict, axis=0)\n",
    "target = np.concatenate(target, axis=0)\n",
    "snrss = np.concatenate(snrss, axis=0)\n",
    "\n",
    "gru_scr4_res['gru_scr4_prd'] = predict\n",
    "gru_scr4_res['gru_scr4_trg'] = target\n",
    "gru_scr4_res['gru_scr4_snr'] = snrss * scr4_std + scr4_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Link 143/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.35it/s]\n",
      "Link 144/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.33it/s]\n",
      "Link 145/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.50it/s]\n",
      "Link 146/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.78it/s]\n",
      "Link 147/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  7.81it/s]\n",
      "Link 148/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  7.19it/s]\n",
      "Link 149/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.47it/s]\n",
      "Link 150/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.68it/s]\n",
      "Link 151/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  8.54it/s]\n",
      "Link 152/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.83it/s]\n",
      "Link 153/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.72it/s]\n",
      "Link 154/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.99it/s]\n",
      "Link 155/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.81it/s]\n",
      "Link 156/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.75it/s]\n",
      "Link 157/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.27it/s]\n",
      "Link 158/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.87it/s]\n",
      "Link 159/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "Link 160/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  8.80it/s]\n",
      "Link 161/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.79it/s]\n",
      "Link 162/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.19it/s]\n",
      "Link 163/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.67it/s]\n",
      "Link 164/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.08it/s]\n",
      "Link 165/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.78it/s]\n",
      "Link 166/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 13.47it/s]\n",
      "Link 167/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.81it/s]\n",
      "Link 168/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.67it/s]\n",
      "Link 169/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.79it/s]\n",
      "Link 170/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.78it/s]\n",
      "Link 171/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.05it/s]\n",
      "Link 172/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.85it/s]\n",
      "Link 173/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 11.23it/s]\n",
      "Link 174/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  8.94it/s]\n",
      "Link 175/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.94it/s]\n",
      "Link 176/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.86it/s]\n",
      "Link 177/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.57it/s]\n",
      "Link 178/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.02it/s]\n",
      "Link 179/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 11.70it/s]\n",
      "Link 180/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  8.43it/s]\n",
      "Link 181/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.81it/s]\n",
      "Link 182/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 18.23it/s]\n",
      "Link 183/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "Link 184/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  5.99it/s]\n",
      "Link 185/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 13.72it/s]\n",
      "Link 186/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 18.03it/s]\n",
      "Link 187/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  8.59it/s]\n",
      "Link 188/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.92it/s]\n",
      "Link 189/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.90it/s]\n",
      "Link 190/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 13.29it/s]\n",
      "Link 191/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 17.43it/s]\n",
      "Link 192/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  6.70it/s]\n",
      "Link 193/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.92it/s]\n",
      "Link 194/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.93it/s]\n",
      "Link 195/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  5.36it/s]\n",
      "Link 196/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.78it/s]\n",
      "Link 197/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "Link 198/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 12.71it/s]\n",
      "Link 199/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 11.97it/s]\n",
      "Link 200/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.78it/s]\n",
      "Link 201/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.72it/s]\n",
      "Link 202/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "Link 203/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.70it/s]\n",
      "Link 204/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "Link 205/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 12.57it/s]\n",
      "Link 206/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 10.36it/s]\n",
      "Link 207/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 14.23it/s]\n",
      "Link 208/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.86it/s]\n",
      "Link 209/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.94it/s]\n",
      "Link 210/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.68it/s]\n",
      "Link 211/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.82it/s]\n",
      "Link 212/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.24it/s]\n",
      "Link 213/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.13it/s]\n",
      "Link 214/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.88it/s]\n",
      "Link 215/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.04it/s]\n",
      "Link 216/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.95it/s]\n",
      "Link 217/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  6.14it/s]\n",
      "Link 218/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.85it/s]\n",
      "Link 219/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.46it/s]\n",
      "Link 220/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 15.72it/s]\n",
      "Link 221/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  7.51it/s]\n",
      "Link 222/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 14.40it/s]\n",
      "Link 223/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "Link 224/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.26it/s]\n",
      "Link 225/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "Link 226/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.55it/s]\n",
      "Link 227/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 15.82it/s]\n",
      "Link 228/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "Link 229/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  8.16it/s]\n",
      "Link 230/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.74it/s]\n",
      "Link 231/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.36it/s]\n",
      "Link 232/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.97it/s]\n",
      "Link 233/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.93it/s]\n",
      "Link 234/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.34it/s]\n",
      "Link 235/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.45it/s]\n",
      "Link 236/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 10.99it/s]\n",
      "Link 237/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.85it/s]\n",
      "Link 238/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.83it/s]\n",
      "Link 239/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.93it/s]\n",
      "Link 240/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.44it/s]\n",
      "Link 241/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  6.84it/s]\n",
      "Link 242/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  7.59it/s]\n",
      "Link 243/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.30it/s]\n",
      "Link 244/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.12it/s]\n",
      "Link 245/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.89it/s]\n",
      "Link 246/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.59it/s]\n",
      "Link 247/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 14.06it/s]\n",
      "Link 248/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  8.08it/s]\n",
      "Link 249/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.71it/s]\n",
      "Link 250/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.02it/s]\n",
      "Link 251/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.87it/s]\n",
      "Link 252/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.33it/s]\n",
      "Link 253/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.04it/s]\n",
      "Link 254/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 10.28it/s]\n",
      "Link 255/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.89it/s]\n",
      "Link 256/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.44it/s]\n",
      "Link 257/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\n",
      "Link 258/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.84it/s]\n",
      "Link 259/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 11.98it/s]\n",
      "Link 260/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.85it/s]\n",
      "Link 261/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 10.20it/s]\n",
      "Link 262/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  7.95it/s]\n",
      "Link 263/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.23it/s]\n",
      "Link 264/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.71it/s]\n",
      "Link 265/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  5.83it/s]\n",
      "Link 266/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.02it/s]\n",
      "Link 267/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 14.86it/s]\n",
      "Link 268/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.17it/s]\n",
      "Link 269/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 15.97it/s]\n",
      "Link 270/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.71it/s]\n",
      "Link 271/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  5.52it/s]\n",
      "Link 272/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.97it/s]\n",
      "Link 273/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.20it/s]\n",
      "Link 274/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.88it/s]\n",
      "Link 275/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.16it/s]\n",
      "Link 276/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.85it/s]\n",
      "Link 277/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.01it/s]\n",
      "Link 278/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.95it/s]\n",
      "Link 279/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.11it/s]\n",
      "Link 280/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.81it/s]\n",
      "Link 281/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.31it/s]\n",
      "Link 282/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.86it/s]\n",
      "Link 283/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.67it/s]\n",
      "Link 284/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test Accuracy per Link\n",
    "\n",
    "accr = []\n",
    "for i in range(total_links - test_start):\n",
    "    test_data_i = [link_data[i+test_start]]\n",
    "    test_x_i = torch.cat(tuple(link[0] for link in test_data_i),dim=0)\n",
    "    test_y_i = torch.cat(tuple(link[1] for link in test_data_i),dim=0)\n",
    "    \n",
    "    test_dataloader = data.DataLoader(\n",
    "    data.TensorDataset(test_x_i,test_y_i), \n",
    "    batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=16, pin_memory=True)\n",
    "    \n",
    "    progress_test_epoch = tqdm(\n",
    "        test_data_i, \n",
    "        desc=f'Link {i+test_start+1}/{total_links}, Test',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(test_data_i), smoothing=.9)\n",
    "\n",
    "    predict = []\n",
    "    target = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (sentence, tags) in enumerate(progress_test_epoch):\n",
    "            sentence = sentence.cuda()\n",
    "            tags = tags.cuda()\n",
    "            tag_scores = model(sentence)\n",
    "            predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "            target.append(tags.cpu().numpy())\n",
    "\n",
    "    predict = np.concatenate(predict, axis=0)\n",
    "    target = np.concatenate(target, axis=0)\n",
    "\n",
    "    tp = predict[target==1].sum()\n",
    "    tn = (target==0).sum() - predict[target==0].sum()\n",
    "    fp = predict[target==0].sum()\n",
    "    fn = (target==1).sum() - predict[target==1].sum()\n",
    "    accr.append((tp+tn)/(tp+tn+fp+fn))    \n",
    "\n",
    "gru_scr4_res['gru_scr4_accr_perlink'] = accr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training: 100%|█████████████████████████| 2468/2468 [01:51<00:00, 22.18it/s]\n",
      "Epoch 1/10, Validation: 100%|███████████████████████| 4757/4757 [02:09<00:00, 36.69it/s]\n",
      "Epoch 2/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.52000, validation loss: 0.47813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Training: 100%|█████████████████████████| 2468/2468 [02:11<00:00, 18.73it/s]\n",
      "Epoch 2/10, Validation: 100%|███████████████████████| 4757/4757 [02:28<00:00, 32.00it/s]\n",
      "Epoch 3/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.45005, validation loss: 0.41727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Training: 100%|█████████████████████████| 2468/2468 [01:47<00:00, 22.86it/s]\n",
      "Epoch 3/10, Validation: 100%|███████████████████████| 4757/4757 [02:03<00:00, 38.44it/s]\n",
      "Epoch 4/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.43178, validation loss: 0.40852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Training: 100%|█████████████████████████| 2468/2468 [01:49<00:00, 22.50it/s]\n",
      "Epoch 4/10, Validation: 100%|███████████████████████| 4757/4757 [02:06<00:00, 37.53it/s]\n",
      "Epoch 5/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.42661, validation loss: 0.40251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Training: 100%|█████████████████████████| 2468/2468 [01:48<00:00, 22.84it/s]\n",
      "Epoch 5/10, Validation: 100%|███████████████████████| 4757/4757 [02:05<00:00, 38.01it/s]\n",
      "Epoch 6/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.42209, validation loss: 0.39718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Training: 100%|█████████████████████████| 2468/2468 [01:48<00:00, 22.81it/s]\n",
      "Epoch 6/10, Validation: 100%|███████████████████████| 4757/4757 [02:05<00:00, 37.88it/s]\n",
      "Epoch 7/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.41867, validation loss: 0.39369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Training: 100%|█████████████████████████| 2468/2468 [01:51<00:00, 22.15it/s]\n",
      "Epoch 7/10, Validation: 100%|███████████████████████| 4757/4757 [02:08<00:00, 37.07it/s]\n",
      "Epoch 8/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.41667, validation loss: 0.39206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Training: 100%|█████████████████████████| 2468/2468 [01:47<00:00, 22.93it/s]\n",
      "Epoch 8/10, Validation: 100%|███████████████████████| 4757/4757 [02:04<00:00, 38.22it/s]\n",
      "Epoch 9/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.41534, validation loss: 0.39123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Training: 100%|█████████████████████████| 2468/2468 [01:51<00:00, 22.22it/s]\n",
      "Epoch 9/10, Validation: 100%|███████████████████████| 4757/4757 [02:08<00:00, 37.08it/s]\n",
      "Epoch 10/10, Validation:   0%|                                 | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.41430, validation loss: 0.39090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Training: 100%|████████████████████████| 2468/2468 [01:53<00:00, 21.78it/s]\n",
      "Epoch 10/10, Validation: 100%|██████████████████████| 4757/4757 [02:09<00:00, 36.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.41339, validation loss: 0.39069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "model = BGRUTagger(EMBEDDING_DIM, HIDDEN_DIM, TAGSET_SIZE).cuda()\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor([1.0,1.0])).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), 0.0001)\n",
    "\n",
    "train_dataloader=data.DataLoader(data.TensorDataset(train_x,train_y),batch_size=TRAIN_BATCH_SIZE, \n",
    "                                   shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "val_dataloader=data.DataLoader(data.TensorDataset(val_x,val_y),batch_size=VAL_BATCH_SIZE, \n",
    "                                 shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "test_dataloader=data.DataLoader(data.TensorDataset(test_x,test_y), batch_size=VAL_BATCH_SIZE,\n",
    "                                  shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "for epoch_idx in range(NUM_EPOCHS): \n",
    "\n",
    "    progress_training_epoch = tqdm(\n",
    "        train_dataloader, \n",
    "        desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Training',\n",
    "        miniters=1, ncols=88, position=0,\n",
    "        leave=True, total=len(train_dataloader), smoothing=.9)\n",
    "\n",
    "    progress_validation_epoch = tqdm(\n",
    "        val_dataloader, \n",
    "        desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Validation',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(val_dataloader), smoothing=.9)\n",
    "\n",
    "    train_loss = 0\n",
    "    train_size = 0\n",
    "    model.train()\n",
    "    for idx, (sentence, tags) in enumerate(progress_training_epoch):\n",
    "        sentence = sentence.cuda()\n",
    "        tags = tags.cuda()\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(sentence)\n",
    "        loss = loss_function(tag_scores, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss * tags.size()[0]\n",
    "        train_size += tags.size()[0]\n",
    "\n",
    "    test_loss = 0\n",
    "    test_size = 0\n",
    "    predict = []\n",
    "    target = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (sentence, tags) in enumerate(progress_validation_epoch):\n",
    "            sentence = sentence.cuda()\n",
    "            tags = tags.cuda()\n",
    "            tag_scores = model(sentence)\n",
    "            loss = loss_function(tag_scores, tags)\n",
    "            predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "            target.append(tags.cpu().numpy())        \n",
    "            test_loss += loss * tags.size()[0]\n",
    "            test_size += tags.size()[0]\n",
    "    predict = np.concatenate(predict, axis=0)\n",
    "    target = np.concatenate(target, axis=0)\n",
    "\n",
    "    print(f'train loss:{train_loss.item()/train_size: .5f}, '\n",
    "          f'validation loss:{test_loss.item()/test_size: .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test: 100%|█████████████████████████| 26457/26457 [01:26<00:00, 305.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "progress_test_epoch = tqdm(\n",
    "    test_dataloader, \n",
    "    desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Test',\n",
    "    miniters=1, ncols=88, position=0, \n",
    "    leave=True, total=len(test_dataloader), smoothing=.9)\n",
    "\n",
    "predict = []\n",
    "target = []\n",
    "snrss = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (sentence, tags) in enumerate(progress_test_epoch):\n",
    "        sentence = sentence.cuda()\n",
    "        tags = tags.cuda()\n",
    "        tag_scores = model(sentence)\n",
    "        predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "        target.append(tags.cpu().numpy())\n",
    "        snrss.append(sentence.cpu().numpy()[:,0])\n",
    "        \n",
    "predict = np.concatenate(predict, axis=0)\n",
    "target = np.concatenate(target, axis=0)\n",
    "snrss = np.concatenate(snrss, axis=0)\n",
    "\n",
    "bgru_scr4_res['bgru_scr4_prd'] = predict\n",
    "bgru_scr4_res['bgru_scr4_trg'] = target\n",
    "bgru_scr4_res['bgru_scr4_snr'] = snrss * scr4_std + scr4_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Link 143/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.18it/s]\n",
      "Link 144/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "Link 145/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "Link 146/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "Link 147/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.80it/s]\n",
      "Link 148/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.21it/s]\n",
      "Link 149/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.90it/s]\n",
      "Link 150/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.97it/s]\n",
      "Link 151/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.47it/s]\n",
      "Link 152/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "Link 153/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.33it/s]\n",
      "Link 154/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "Link 155/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "Link 156/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "Link 157/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.21it/s]\n",
      "Link 158/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "Link 159/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.77it/s]\n",
      "Link 160/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.24it/s]\n",
      "Link 161/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "Link 162/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.66it/s]\n",
      "Link 163/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      "Link 164/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "Link 165/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "Link 166/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  6.98it/s]\n",
      "Link 167/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "Link 168/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "Link 169/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "Link 170/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "Link 171/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "Link 172/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "Link 173/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  5.87it/s]\n",
      "Link 174/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.66it/s]\n",
      "Link 175/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.14it/s]\n",
      "Link 176/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "Link 177/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.89it/s]\n",
      "Link 178/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.71it/s]\n",
      "Link 179/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  6.11it/s]\n",
      "Link 180/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.36it/s]\n",
      "Link 181/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "Link 182/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  9.44it/s]\n",
      "Link 183/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "Link 184/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.11it/s]\n",
      "Link 185/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  6.97it/s]\n",
      "Link 186/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  9.23it/s]\n",
      "Link 187/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.32it/s]\n",
      "Link 188/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "Link 189/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "Link 190/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  7.55it/s]\n",
      "Link 191/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  9.50it/s]\n",
      "Link 192/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.63it/s]\n",
      "Link 193/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "Link 194/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.53it/s]\n",
      "Link 195/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.78it/s]\n",
      "Link 196/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  8.78it/s]\n",
      "Link 197/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "Link 198/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  6.64it/s]\n",
      "Link 199/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  6.22it/s]\n",
      "Link 200/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "Link 201/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "Link 202/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "Link 203/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.01it/s]\n",
      "Link 204/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "Link 205/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  8.94it/s]\n",
      "Link 206/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  5.61it/s]\n",
      "Link 207/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  7.47it/s]\n",
      "Link 208/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.53it/s]\n",
      "Link 209/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.53it/s]\n",
      "Link 210/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "Link 211/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "Link 212/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "Link 213/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "Link 214/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "Link 215/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "Link 216/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "Link 217/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.24it/s]\n",
      "Link 218/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "Link 219/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  7.79it/s]\n",
      "Link 220/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  7.94it/s]\n",
      "Link 221/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.87it/s]\n",
      "Link 222/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  6.54it/s]\n",
      "Link 223/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "Link 224/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.20it/s]\n",
      "Link 225/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "Link 226/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "Link 227/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  8.30it/s]\n",
      "Link 228/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "Link 229/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.83it/s]\n",
      "Link 230/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "Link 231/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.71it/s]\n",
      "Link 232/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.53it/s]\n",
      "Link 233/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "Link 234/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.82it/s]\n",
      "Link 235/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "Link 236/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  5.71it/s]\n",
      "Link 237/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "Link 238/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.94it/s]\n",
      "Link 239/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "Link 240/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "Link 241/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.56it/s]\n",
      "Link 242/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.90it/s]\n",
      "Link 243/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "Link 244/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "Link 245/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "Link 246/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.76it/s]\n",
      "Link 247/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  8.42it/s]\n",
      "Link 248/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.23it/s]\n",
      "Link 249/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "Link 250/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "Link 251/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "Link 252/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.36it/s]\n",
      "Link 253/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "Link 254/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  5.02it/s]\n",
      "Link 255/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "Link 256/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.77it/s]\n",
      "Link 257/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "Link 258/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.92it/s]\n",
      "Link 259/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  6.11it/s]\n",
      "Link 260/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.49it/s]\n",
      "Link 261/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  5.36it/s]\n",
      "Link 262/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.30it/s]\n",
      "Link 263/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "Link 264/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "Link 265/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.01it/s]\n",
      "Link 266/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "Link 267/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  7.66it/s]\n",
      "Link 268/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "Link 269/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  8.35it/s]\n",
      "Link 270/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "Link 271/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.86it/s]\n",
      "Link 272/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "Link 273/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "Link 274/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "Link 275/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "Link 276/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "Link 277/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "Link 278/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "Link 279/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "Link 280/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.96it/s]\n",
      "Link 281/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "Link 282/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      "Link 283/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "Link 284/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test Accuracy per Link\n",
    "\n",
    "accr = []\n",
    "\n",
    "for i in range(total_links - test_start):\n",
    "    test_data_i = [link_data[i+test_start]]\n",
    "    test_x_i = torch.cat(tuple(link[0] for link in test_data_i),dim=0)\n",
    "    test_y_i = torch.cat(tuple(link[1] for link in test_data_i),dim=0)\n",
    "    \n",
    "    test_dataloader = data.DataLoader(\n",
    "    data.TensorDataset(test_x_i,test_y_i), \n",
    "    batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=16, pin_memory=True)\n",
    "    \n",
    "    progress_test_epoch = tqdm(\n",
    "        test_data_i, \n",
    "        desc=f'Link {i+test_start+1}/{total_links}, Test',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(test_data_i), smoothing=.9)\n",
    "\n",
    "    predict = []\n",
    "    target = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (sentence, tags) in enumerate(progress_test_epoch):\n",
    "            sentence = sentence.cuda()\n",
    "            tags = tags.cuda()\n",
    "            tag_scores = model(sentence)\n",
    "            predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "            target.append(tags.cpu().numpy())\n",
    "\n",
    "    predict = np.concatenate(predict, axis=0)\n",
    "    target = np.concatenate(target, axis=0)\n",
    "\n",
    "    tp = predict[target==1].sum()\n",
    "    tn = (target==0).sum() - predict[target==0].sum()\n",
    "    fp = predict[target==0].sum()\n",
    "    fn = (target==1).sum() - predict[target==1].sum()\n",
    "    accr.append((tp+tn)/(tp+tn+fp+fn))    \n",
    "\n",
    "bgru_scr4_res['bgru_scr4_accr_perlink'] = accr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BSRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training: 100%|████████████████████████| 2468/2468 [00:20<00:00, 121.64it/s]\n",
      "Epoch 1/10, Validation: 100%|██████████████████████| 4757/4757 [00:26<00:00, 180.42it/s]\n",
      "Epoch 2/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.50173, validation loss: 0.46138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Training: 100%|████████████████████████| 2468/2468 [00:14<00:00, 164.74it/s]\n",
      "Epoch 2/10, Validation: 100%|██████████████████████| 4757/4757 [00:20<00:00, 228.79it/s]\n",
      "Epoch 3/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.45479, validation loss: 0.44432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Training: 100%|████████████████████████| 2468/2468 [00:15<00:00, 162.98it/s]\n",
      "Epoch 3/10, Validation: 100%|██████████████████████| 4757/4757 [00:21<00:00, 222.27it/s]\n",
      "Epoch 4/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.44052, validation loss: 0.43466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Training: 100%|████████████████████████| 2468/2468 [00:15<00:00, 164.52it/s]\n",
      "Epoch 4/10, Validation: 100%|██████████████████████| 4757/4757 [00:21<00:00, 222.20it/s]\n",
      "Epoch 5/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.43245, validation loss: 0.42735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Training: 100%|████████████████████████| 2468/2468 [00:15<00:00, 164.44it/s]\n",
      "Epoch 5/10, Validation: 100%|██████████████████████| 4757/4757 [00:20<00:00, 226.85it/s]\n",
      "Epoch 6/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.42601, validation loss: 0.42167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Training: 100%|████████████████████████| 2468/2468 [00:14<00:00, 165.16it/s]\n",
      "Epoch 6/10, Validation: 100%|██████████████████████| 4757/4757 [00:21<00:00, 223.73it/s]\n",
      "Epoch 7/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.42126, validation loss: 0.41726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Training: 100%|████████████████████████| 2468/2468 [00:15<00:00, 161.60it/s]\n",
      "Epoch 7/10, Validation: 100%|██████████████████████| 4757/4757 [00:21<00:00, 221.57it/s]\n",
      "Epoch 8/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.41780, validation loss: 0.41343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Training: 100%|████████████████████████| 2468/2468 [00:15<00:00, 162.59it/s]\n",
      "Epoch 8/10, Validation: 100%|██████████████████████| 4757/4757 [00:21<00:00, 223.94it/s]\n",
      "Epoch 9/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.41537, validation loss: 0.41120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Training: 100%|████████████████████████| 2468/2468 [00:15<00:00, 164.03it/s]\n",
      "Epoch 9/10, Validation: 100%|██████████████████████| 4757/4757 [00:21<00:00, 222.55it/s]\n",
      "Epoch 10/10, Validation:   0%|                                 | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.41357, validation loss: 0.40897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Training: 100%|███████████████████████| 2468/2468 [00:15<00:00, 162.59it/s]\n",
      "Epoch 10/10, Validation: 100%|█████████████████████| 4757/4757 [00:21<00:00, 222.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.41199, validation loss: 0.40753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "model = BSRUTagger(EMBEDDING_DIM, HIDDEN_DIM, TAGSET_SIZE).cuda()\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor([1.0,1.0])).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), 0.0001)\n",
    "\n",
    "train_dataloader=data.DataLoader(data.TensorDataset(train_x,train_y),batch_size=TRAIN_BATCH_SIZE, \n",
    "                                   shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "val_dataloader=data.DataLoader(data.TensorDataset(val_x,val_y),batch_size=VAL_BATCH_SIZE, \n",
    "                                 shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "test_dataloader=data.DataLoader(data.TensorDataset(test_x,test_y), batch_size=VAL_BATCH_SIZE,\n",
    "                                  shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "for epoch_idx in range(NUM_EPOCHS): \n",
    "\n",
    "    progress_training_epoch = tqdm(\n",
    "        train_dataloader, \n",
    "        desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Training',\n",
    "        miniters=1, ncols=88, position=0,\n",
    "        leave=True, total=len(train_dataloader), smoothing=.9)\n",
    "\n",
    "    progress_validation_epoch = tqdm(\n",
    "        val_dataloader, \n",
    "        desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Validation',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(val_dataloader), smoothing=.9)\n",
    "\n",
    "    train_loss = 0\n",
    "    train_size = 0\n",
    "    model.train()\n",
    "    for idx, (sentence, tags) in enumerate(progress_training_epoch):\n",
    "        sentence = sentence.cuda()\n",
    "        tags = tags.cuda()\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(sentence)\n",
    "        loss = loss_function(tag_scores, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss * tags.size()[0]\n",
    "        train_size += tags.size()[0]\n",
    "\n",
    "    test_loss = 0\n",
    "    test_size = 0\n",
    "    predict = []\n",
    "    target = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (sentence, tags) in enumerate(progress_validation_epoch):\n",
    "            sentence = sentence.cuda()\n",
    "            tags = tags.cuda()\n",
    "            tag_scores = model(sentence)\n",
    "            loss = loss_function(tag_scores, tags)\n",
    "            predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "            target.append(tags.cpu().numpy())        \n",
    "            test_loss += loss * tags.size()[0]\n",
    "            test_size += tags.size()[0]\n",
    "    predict = np.concatenate(predict, axis=0)\n",
    "    target = np.concatenate(target, axis=0)\n",
    "\n",
    "    print(f'train loss:{train_loss.item()/train_size: .5f}, '\n",
    "          f'validation loss:{test_loss.item()/test_size: .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test: 100%|█████████████████████████| 26457/26457 [00:30<00:00, 869.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "progress_test_epoch = tqdm(\n",
    "    test_dataloader, \n",
    "    desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Test',\n",
    "    miniters=1, ncols=88, position=0, \n",
    "    leave=True, total=len(test_dataloader), smoothing=.9)\n",
    "\n",
    "predict = []\n",
    "target = []\n",
    "snrss = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (sentence, tags) in enumerate(progress_test_epoch):\n",
    "        sentence = sentence.cuda()\n",
    "        tags = tags.cuda()\n",
    "        tag_scores = model(sentence)\n",
    "        predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "        target.append(tags.cpu().numpy())\n",
    "        snrss.append(sentence.cpu().numpy()[:,0])\n",
    "        \n",
    "predict = np.concatenate(predict, axis=0)\n",
    "target = np.concatenate(target, axis=0)\n",
    "snrss = np.concatenate(snrss, axis=0)\n",
    "\n",
    "bsru_scr4_res['bsru_scr4_prd'] = predict\n",
    "bsru_scr4_res['bsru_scr4_trg'] = target\n",
    "bsru_scr4_res['bsru_scr4_snr'] = snrss * scr4_std + scr4_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Link 143/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 23.24it/s]\n",
      "Link 144/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 17.93it/s]\n",
      "Link 145/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 24.06it/s]\n",
      "Link 146/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 15.29it/s]\n",
      "Link 147/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 52.82it/s]\n",
      "Link 148/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 47.31it/s]\n",
      "Link 149/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 21.20it/s]\n",
      "Link 150/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 22.89it/s]\n",
      "Link 151/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 49.68it/s]\n",
      "Link 152/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.65it/s]\n",
      "Link 153/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 15.86it/s]\n",
      "Link 154/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 17.36it/s]\n",
      "Link 155/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.18it/s]\n",
      "Link 156/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 15.89it/s]\n",
      "Link 157/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 24.84it/s]\n",
      "Link 158/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.72it/s]\n",
      "Link 159/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 20.31it/s]\n",
      "Link 160/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 51.06it/s]\n",
      "Link 161/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.44it/s]\n",
      "Link 162/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 18.57it/s]\n",
      "Link 163/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 21.69it/s]\n",
      "Link 164/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 17.70it/s]\n",
      "Link 165/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 15.99it/s]\n",
      "Link 166/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 74.63it/s]\n",
      "Link 167/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.26it/s]\n",
      "Link 168/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.11it/s]\n",
      "Link 169/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.33it/s]\n",
      "Link 170/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.89it/s]\n",
      "Link 171/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 17.60it/s]\n",
      "Link 172/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.55it/s]\n",
      "Link 173/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 64.73it/s]\n",
      "Link 174/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 51.55it/s]\n",
      "Link 175/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 23.86it/s]\n",
      "Link 176/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.90it/s]\n",
      "Link 177/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 20.95it/s]\n",
      "Link 178/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 18.97it/s]\n",
      "Link 179/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 66.45it/s]\n",
      "Link 180/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 47.95it/s]\n",
      "Link 181/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.40it/s]\n",
      "Link 182/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 99.48it/s]\n",
      "Link 183/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 21.36it/s]\n",
      "Link 184/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 34.39it/s]\n",
      "Link 185/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 76.32it/s]\n",
      "Link 186/284, Test: 100%|████████████████████████████████| 1/1 [00:00<00:00, 101.79it/s]\n",
      "Link 187/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 48.68it/s]\n",
      "Link 188/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.80it/s]\n",
      "Link 189/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 11.45it/s]\n",
      "Link 190/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 80.93it/s]\n",
      "Link 191/284, Test: 100%|████████████████████████████████| 1/1 [00:00<00:00, 102.12it/s]\n",
      "Link 192/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 39.99it/s]\n",
      "Link 193/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 17.55it/s]\n",
      "Link 194/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.90it/s]\n",
      "Link 195/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 30.38it/s]\n",
      "Link 196/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 93.44it/s]\n",
      "Link 197/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 15.36it/s]\n",
      "Link 198/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 73.40it/s]\n",
      "Link 199/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 68.56it/s]\n",
      "Link 200/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.78it/s]\n",
      "Link 201/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.59it/s]\n",
      "Link 202/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 15.83it/s]\n",
      "Link 203/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 22.27it/s]\n",
      "Link 204/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 15.99it/s]\n",
      "Link 205/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 97.41it/s]\n",
      "Link 206/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 61.07it/s]\n",
      "Link 207/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 81.11it/s]\n",
      "Link 208/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.92it/s]\n",
      "Link 209/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.98it/s]\n",
      "Link 210/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 15.77it/s]\n",
      "Link 211/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.30it/s]\n",
      "Link 212/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 18.56it/s]\n",
      "Link 213/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 18.21it/s]\n",
      "Link 214/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.81it/s]\n",
      "Link 215/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 17.67it/s]\n",
      "Link 216/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.98it/s]\n",
      "Link 217/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 35.34it/s]\n",
      "Link 218/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.43it/s]\n",
      "Link 219/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 96.53it/s]\n",
      "Link 220/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 88.02it/s]\n",
      "Link 221/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 43.16it/s]\n",
      "Link 222/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 80.85it/s]\n",
      "Link 223/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 15.60it/s]\n",
      "Link 224/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 25.08it/s]\n",
      "Link 225/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.20it/s]\n",
      "Link 226/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.68it/s]\n",
      "Link 227/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 90.64it/s]\n",
      "Link 228/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 17.08it/s]\n",
      "Link 229/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 54.54it/s]\n",
      "Link 230/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 15.95it/s]\n",
      "Link 231/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 19.42it/s]\n",
      "Link 232/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 17.46it/s]\n",
      "Link 233/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 17.21it/s]\n",
      "Link 234/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 20.46it/s]\n",
      "Link 235/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 20.41it/s]\n",
      "Link 236/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 63.02it/s]\n",
      "Link 237/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.55it/s]\n",
      "Link 238/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 22.16it/s]\n",
      "Link 239/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 17.23it/s]\n",
      "Link 240/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 26.94it/s]\n",
      "Link 241/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 39.69it/s]\n",
      "Link 242/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 43.86it/s]\n",
      "Link 243/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 19.03it/s]\n",
      "Link 244/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 18.01it/s]\n",
      "Link 245/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.82it/s]\n",
      "Link 246/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 30.77it/s]\n",
      "Link 247/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 92.05it/s]\n",
      "Link 248/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 46.93it/s]\n",
      "Link 249/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 15.83it/s]\n",
      "Link 250/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 17.72it/s]\n",
      "Link 251/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.98it/s]\n",
      "Link 252/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 26.52it/s]\n",
      "Link 253/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 17.91it/s]\n",
      "Link 254/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 59.49it/s]\n",
      "Link 255/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.69it/s]\n",
      "Link 256/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 19.89it/s]\n",
      "Link 257/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 19.37it/s]\n",
      "Link 258/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 22.34it/s]\n",
      "Link 259/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 69.20it/s]\n",
      "Link 260/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 28.05it/s]\n",
      "Link 261/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 59.27it/s]\n",
      "Link 262/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 48.63it/s]\n",
      "Link 263/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 18.74it/s]\n",
      "Link 264/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 21.89it/s]\n",
      "Link 265/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 33.64it/s]\n",
      "Link 266/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 17.36it/s]\n",
      "Link 267/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 83.87it/s]\n",
      "Link 268/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 18.17it/s]\n",
      "Link 269/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 90.92it/s]\n",
      "Link 270/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 15.92it/s]\n",
      "Link 271/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 31.81it/s]\n",
      "Link 272/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 17.20it/s]\n",
      "Link 273/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 18.52it/s]\n",
      "Link 274/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.69it/s]\n",
      "Link 275/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 18.44it/s]\n",
      "Link 276/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 17.21it/s]\n",
      "Link 277/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 17.54it/s]\n",
      "Link 278/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 16.95it/s]\n",
      "Link 279/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 17.92it/s]\n",
      "Link 280/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 22.10it/s]\n",
      "Link 281/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 19.06it/s]\n",
      "Link 282/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 22.28it/s]\n",
      "Link 283/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 27.03it/s]\n",
      "Link 284/284, Test: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 17.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test Accuracy per Link\n",
    "\n",
    "accr = []\n",
    "\n",
    "for i in range(total_links - test_start):\n",
    "    test_data_i = [link_data[i+test_start]]\n",
    "    test_x_i = torch.cat(tuple(link[0] for link in test_data_i),dim=0)\n",
    "    test_y_i = torch.cat(tuple(link[1] for link in test_data_i),dim=0)\n",
    "    \n",
    "    test_dataloader = data.DataLoader(\n",
    "    data.TensorDataset(test_x_i,test_y_i), \n",
    "    batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=16, pin_memory=True)\n",
    "    \n",
    "    progress_test_epoch = tqdm(\n",
    "        test_data_i, \n",
    "        desc=f'Link {i+test_start+1}/{total_links}, Test',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(test_data_i), smoothing=.9)\n",
    "\n",
    "    predict = []\n",
    "    target = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (sentence, tags) in enumerate(progress_test_epoch):\n",
    "            sentence = sentence.cuda()\n",
    "            tags = tags.cuda()\n",
    "            tag_scores = model(sentence)\n",
    "            predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "            target.append(tags.cpu().numpy())\n",
    "\n",
    "    predict = np.concatenate(predict, axis=0)\n",
    "    target = np.concatenate(target, axis=0)\n",
    "\n",
    "    tp = predict[target==1].sum()\n",
    "    tn = (target==0).sum() - predict[target==0].sum()\n",
    "    fp = predict[target==0].sum()\n",
    "    fn = (target==1).sum() - predict[target==1].sum()\n",
    "    accr.append((tp+tn)/(tp+tn+fp+fn))    \n",
    "\n",
    "bsru_scr4_res['bsru_scr4_accr_perlink'] = accr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBSDNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/10, Training: 100%|████████████████████████| 2468/2468 [00:19<00:00, 125.35it/s]\n",
      "Epoch 0/10, Validation: 100%|██████████████████████| 4757/4757 [00:27<00:00, 173.23it/s]\n",
      "Epoch 1/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.51335, validation loss: 0.47881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training: 100%|████████████████████████| 2468/2468 [00:19<00:00, 123.53it/s]\n",
      "Epoch 1/10, Validation: 100%|██████████████████████| 4757/4757 [00:27<00:00, 170.19it/s]\n",
      "Epoch 2/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.45970, validation loss: 0.44166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Training: 100%|████████████████████████| 2468/2468 [00:19<00:00, 124.93it/s]\n",
      "Epoch 2/10, Validation: 100%|██████████████████████| 4757/4757 [00:27<00:00, 172.42it/s]\n",
      "Epoch 3/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.44089, validation loss: 0.42344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Training: 100%|████████████████████████| 2468/2468 [00:19<00:00, 125.12it/s]\n",
      "Epoch 3/10, Validation: 100%|██████████████████████| 4757/4757 [00:28<00:00, 169.24it/s]\n",
      "Epoch 4/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.43180, validation loss: 0.41654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Training: 100%|████████████████████████| 2468/2468 [00:19<00:00, 126.53it/s]\n",
      "Epoch 4/10, Validation: 100%|██████████████████████| 4757/4757 [00:27<00:00, 172.00it/s]\n",
      "Epoch 5/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.42681, validation loss: 0.41245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Training: 100%|████████████████████████| 2468/2468 [00:19<00:00, 125.09it/s]\n",
      "Epoch 5/10, Validation: 100%|██████████████████████| 4757/4757 [00:28<00:00, 168.64it/s]\n",
      "Epoch 6/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.42269, validation loss: 0.40935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Training: 100%|████████████████████████| 2468/2468 [00:20<00:00, 123.29it/s]\n",
      "Epoch 6/10, Validation: 100%|██████████████████████| 4757/4757 [00:28<00:00, 168.92it/s]\n",
      "Epoch 7/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.41934, validation loss: 0.40768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Training: 100%|████████████████████████| 2468/2468 [00:20<00:00, 123.37it/s]\n",
      "Epoch 7/10, Validation: 100%|██████████████████████| 4757/4757 [00:27<00:00, 170.84it/s]\n",
      "Epoch 8/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.41666, validation loss: 0.40545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Training: 100%|████████████████████████| 2468/2468 [00:19<00:00, 126.17it/s]\n",
      "Epoch 8/10, Validation: 100%|██████████████████████| 4757/4757 [00:28<00:00, 167.25it/s]\n",
      "Epoch 9/10, Validation:   0%|                                  | 0/4757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.41396, validation loss: 0.40365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Training: 100%|████████████████████████| 2468/2468 [00:19<00:00, 125.46it/s]\n",
      "Epoch 9/10, Validation: 100%|██████████████████████| 4757/4757 [00:27<00:00, 172.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.41118, validation loss: 0.40090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model = CBSDNN().cuda()\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor([1.0,1.0])).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "train_dataloader=data.DataLoader(data.TensorDataset(train_x_cbsdnn,train_y),batch_size=TRAIN_BATCH_SIZE, \n",
    "                                   shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "val_dataloader=data.DataLoader(data.TensorDataset(val_x_cbsdnn,val_y),batch_size=VAL_BATCH_SIZE, \n",
    "                                 shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "test_dataloader=data.DataLoader(data.TensorDataset(test_x_cbsdnn,test_y), batch_size=VAL_BATCH_SIZE,\n",
    "                                  shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch_idx in range(NUM_EPOCHS):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "\n",
    "    progress_training_epoch = tqdm(\n",
    "        train_dataloader, \n",
    "        desc=f'Epoch {epoch_idx}/{NUM_EPOCHS}, Training',\n",
    "        miniters=1, ncols=88, position=0,\n",
    "        leave=True, total=len(train_dataloader), smoothing=.9)\n",
    "\n",
    "    progress_validation_epoch = tqdm(\n",
    "        val_dataloader, \n",
    "        desc=f'Epoch {epoch_idx}/{NUM_EPOCHS}, Validation',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(val_dataloader), smoothing=.9)\n",
    "\n",
    "    train_loss = 0\n",
    "    train_size = 0\n",
    "    model.train()\n",
    "\n",
    "    for idx, (input, target) in enumerate(progress_training_epoch):\n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "        model.zero_grad()\n",
    "        predict = model(input)\n",
    "        loss = loss_function(predict, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss * target.size()[0]\n",
    "        train_size += target.size()[0]\n",
    "\n",
    "    test_loss = 0\n",
    "    test_size = 0\n",
    "    predict = []\n",
    "    target = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (batch_input, batch_target) in enumerate(progress_validation_epoch):\n",
    "            batch_input = batch_input.cuda()\n",
    "            batch_target = batch_target.cuda()\n",
    "            batch_predict = model(batch_input)\n",
    "            loss = loss_function(batch_predict, batch_target)\n",
    "            predict.append(batch_predict.argmax(dim=1).cpu().numpy())\n",
    "            target.append(batch_target.cpu().numpy())        \n",
    "            test_loss += loss * batch_target.size()[0]\n",
    "            test_size += batch_target.size()[0]\n",
    "    predict = np.concatenate(predict, axis=0)\n",
    "    target = np.concatenate(target, axis=0)\n",
    "  \n",
    "    print(f'train loss:{train_loss.item()/train_size: .5f}, '\n",
    "          f'validation loss:{test_loss.item()/test_size: .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test: 100%|█████████████████████████| 26457/26457 [00:38<00:00, 682.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "progress_test_epoch = tqdm(\n",
    "    test_dataloader, \n",
    "    desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Test',\n",
    "    miniters=1, ncols=88, position=0, \n",
    "    leave=True, total=len(test_dataloader), smoothing=.9)\n",
    "\n",
    "predict = []\n",
    "target = []\n",
    "snrss = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (batch_input, batch_target) in enumerate(progress_test_epoch):\n",
    "        batch_input = batch_input.cuda()\n",
    "        batch_target = batch_target.cuda()\n",
    "        batch_predict = model(batch_input)\n",
    "        predict.append(batch_predict.argmax(dim=1).cpu().numpy())\n",
    "        target.append(batch_target.cpu().numpy())        \n",
    "        snrss.append(batch_input.cpu().numpy()[:,0][:,0])\n",
    "predict = np.concatenate(predict, axis=0)\n",
    "target = np.concatenate(target, axis=0)\n",
    "snrss = np.concatenate(snrss, axis=0)\n",
    "\n",
    "cbsdnn_scr4_res['cbsdnn_scr4_prd'] = predict\n",
    "cbsdnn_scr4_res['cbsdnn_scr4_trg'] = target\n",
    "cbsdnn_scr4_res['cbsdnn_scr4_snr'] = snrss * scr4_std + scr4_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Link 143/284, Test: 100%|████████████████████████████| 170/170 [00:01<00:00, 140.06it/s]\n",
      "Link 144/284, Test: 100%|████████████████████████████| 221/221 [00:01<00:00, 177.72it/s]\n",
      "Link 145/284, Test: 100%|████████████████████████████| 165/165 [00:01<00:00, 140.16it/s]\n",
      "Link 146/284, Test: 100%|████████████████████████████| 267/267 [00:01<00:00, 204.17it/s]\n",
      "Link 147/284, Test: 100%|███████████████████████████████| 80/80 [00:01<00:00, 77.29it/s]\n",
      "Link 148/284, Test: 100%|███████████████████████████████| 90/90 [00:01<00:00, 86.50it/s]\n",
      "Link 149/284, Test: 100%|████████████████████████████| 203/203 [00:01<00:00, 157.33it/s]\n",
      "Link 150/284, Test: 100%|████████████████████████████| 189/189 [00:01<00:00, 157.46it/s]\n",
      "Link 151/284, Test: 100%|███████████████████████████████| 86/86 [00:01<00:00, 80.46it/s]\n",
      "Link 152/284, Test: 100%|████████████████████████████| 260/260 [00:01<00:00, 202.25it/s]\n",
      "Link 153/284, Test: 100%|████████████████████████████| 273/273 [00:01<00:00, 208.42it/s]\n",
      "Link 154/284, Test: 100%|████████████████████████████| 250/250 [00:01<00:00, 192.15it/s]\n",
      "Link 155/284, Test: 100%|████████████████████████████| 266/266 [00:01<00:00, 208.66it/s]\n",
      "Link 156/284, Test: 100%|████████████████████████████| 272/272 [00:01<00:00, 207.48it/s]\n",
      "Link 157/284, Test: 100%|████████████████████████████| 174/174 [00:01<00:00, 149.13it/s]\n",
      "Link 158/284, Test: 100%|████████████████████████████| 259/259 [00:01<00:00, 204.03it/s]\n",
      "Link 159/284, Test: 100%|████████████████████████████| 213/213 [00:01<00:00, 174.89it/s]\n",
      "Link 160/284, Test: 100%|███████████████████████████████| 84/84 [00:01<00:00, 78.62it/s]\n",
      "Link 161/284, Test: 100%|████████████████████████████| 264/264 [00:01<00:00, 174.96it/s]\n",
      "Link 162/284, Test: 100%|████████████████████████████| 233/233 [00:01<00:00, 181.95it/s]\n",
      "Link 163/284, Test: 100%|████████████████████████████| 200/200 [00:01<00:00, 161.90it/s]\n",
      "Link 164/284, Test: 100%|████████████████████████████| 244/244 [00:01<00:00, 194.38it/s]\n",
      "Link 165/284, Test: 100%|████████████████████████████| 270/270 [00:01<00:00, 204.63it/s]\n",
      "Link 166/284, Test: 100%|███████████████████████████████| 55/55 [00:01<00:00, 53.94it/s]\n",
      "Link 167/284, Test: 100%|████████████████████████████| 266/266 [00:01<00:00, 200.67it/s]\n",
      "Link 168/284, Test: 100%|████████████████████████████| 268/268 [00:01<00:00, 210.63it/s]\n",
      "Link 169/284, Test: 100%|████████████████████████████| 264/264 [00:01<00:00, 188.44it/s]\n",
      "Link 170/284, Test: 100%|████████████████████████████| 256/256 [00:01<00:00, 195.63it/s]\n",
      "Link 171/284, Test: 100%|████████████████████████████| 246/246 [00:01<00:00, 194.42it/s]\n",
      "Link 172/284, Test: 100%|████████████████████████████| 262/262 [00:01<00:00, 201.64it/s]\n",
      "Link 173/284, Test: 100%|███████████████████████████████| 66/66 [00:01<00:00, 52.38it/s]\n",
      "Link 174/284, Test: 100%|███████████████████████████████| 83/83 [00:01<00:00, 77.34it/s]\n",
      "Link 175/284, Test: 100%|████████████████████████████| 181/181 [00:01<00:00, 153.46it/s]\n",
      "Link 176/284, Test: 100%|████████████████████████████| 256/256 [00:01<00:00, 163.36it/s]\n",
      "Link 177/284, Test: 100%|████████████████████████████| 205/205 [00:01<00:00, 167.13it/s]\n",
      "Link 178/284, Test: 100%|████████████████████████████| 227/227 [00:01<00:00, 182.15it/s]\n",
      "Link 179/284, Test: 100%|███████████████████████████████| 63/63 [00:01<00:00, 51.78it/s]\n",
      "Link 180/284, Test: 100%|███████████████████████████████| 88/88 [00:01<00:00, 83.59it/s]\n",
      "Link 181/284, Test: 100%|████████████████████████████| 263/263 [00:01<00:00, 171.69it/s]\n",
      "Link 182/284, Test: 100%|███████████████████████████████| 40/40 [00:01<00:00, 38.60it/s]\n",
      "Link 183/284, Test: 100%|████████████████████████████| 201/201 [00:01<00:00, 165.95it/s]\n",
      "Link 184/284, Test: 100%|████████████████████████████| 124/124 [00:01<00:00, 114.91it/s]\n",
      "Link 185/284, Test: 100%|███████████████████████████████| 54/54 [00:00<00:00, 54.41it/s]\n",
      "Link 186/284, Test: 100%|███████████████████████████████| 40/40 [00:00<00:00, 40.89it/s]\n",
      "Link 187/284, Test: 100%|███████████████████████████████| 87/87 [00:01<00:00, 83.18it/s]\n",
      "Link 188/284, Test: 100%|████████████████████████████| 256/256 [00:01<00:00, 198.29it/s]\n",
      "Link 189/284, Test: 100%|████████████████████████████| 368/368 [00:01<00:00, 250.77it/s]\n",
      "Link 190/284, Test: 100%|███████████████████████████████| 51/51 [00:01<00:00, 44.78it/s]\n",
      "Link 191/284, Test: 100%|███████████████████████████████| 41/41 [00:00<00:00, 41.28it/s]\n",
      "Link 192/284, Test: 100%|█████████████████████████████| 107/107 [00:01<00:00, 96.16it/s]\n",
      "Link 193/284, Test: 100%|████████████████████████████| 245/245 [00:01<00:00, 184.81it/s]\n",
      "Link 194/284, Test: 100%|████████████████████████████| 255/255 [00:01<00:00, 177.69it/s]\n",
      "Link 195/284, Test: 100%|████████████████████████████| 139/139 [00:01<00:00, 125.33it/s]\n",
      "Link 196/284, Test: 100%|███████████████████████████████| 44/44 [00:00<00:00, 44.69it/s]\n",
      "Link 197/284, Test: 100%|████████████████████████████| 281/281 [00:01<00:00, 214.63it/s]\n",
      "Link 198/284, Test: 100%|███████████████████████████████| 57/57 [00:00<00:00, 57.68it/s]\n",
      "Link 199/284, Test: 100%|███████████████████████████████| 62/62 [00:01<00:00, 60.97it/s]\n",
      "Link 200/284, Test: 100%|████████████████████████████| 258/258 [00:01<00:00, 193.29it/s]\n",
      "Link 201/284, Test: 100%|████████████████████████████| 261/261 [00:01<00:00, 199.63it/s]\n",
      "Link 202/284, Test: 100%|████████████████████████████| 274/274 [00:01<00:00, 205.79it/s]\n",
      "Link 203/284, Test: 100%|████████████████████████████| 193/193 [00:01<00:00, 155.79it/s]\n",
      "Link 204/284, Test: 100%|████████████████████████████| 270/270 [00:01<00:00, 208.66it/s]\n",
      "Link 205/284, Test: 100%|███████████████████████████████| 43/43 [00:01<00:00, 36.66it/s]\n",
      "Link 206/284, Test: 100%|███████████████████████████████| 69/69 [00:01<00:00, 67.96it/s]\n",
      "Link 207/284, Test: 100%|███████████████████████████████| 52/52 [00:00<00:00, 52.30it/s]\n",
      "Link 208/284, Test: 100%|████████████████████████████| 253/253 [00:01<00:00, 195.64it/s]\n",
      "Link 209/284, Test: 100%|████████████████████████████| 253/253 [00:01<00:00, 196.70it/s]\n",
      "Link 210/284, Test: 100%|████████████████████████████| 274/274 [00:01<00:00, 206.01it/s]\n",
      "Link 211/284, Test: 100%|████████████████████████████| 264/264 [00:01<00:00, 206.52it/s]\n",
      "Link 212/284, Test: 100%|████████████████████████████| 229/229 [00:01<00:00, 188.53it/s]\n",
      "Link 213/284, Test: 100%|████████████████████████████| 237/237 [00:01<00:00, 188.27it/s]\n",
      "Link 214/284, Test: 100%|████████████████████████████| 257/257 [00:01<00:00, 202.05it/s]\n",
      "Link 215/284, Test: 100%|████████████████████████████| 245/245 [00:01<00:00, 195.16it/s]\n",
      "Link 216/284, Test: 100%|████████████████████████████| 252/252 [00:01<00:00, 193.69it/s]\n",
      "Link 217/284, Test: 100%|████████████████████████████| 119/119 [00:01<00:00, 107.62it/s]\n",
      "Link 218/284, Test: 100%|████████████████████████████| 263/263 [00:01<00:00, 200.62it/s]\n",
      "Link 219/284, Test: 100%|███████████████████████████████| 43/43 [00:00<00:00, 43.56it/s]\n",
      "Link 220/284, Test: 100%|███████████████████████████████| 48/48 [00:00<00:00, 49.36it/s]\n",
      "Link 221/284, Test: 100%|█████████████████████████████| 100/100 [00:01<00:00, 94.69it/s]\n",
      "Link 222/284, Test: 100%|███████████████████████████████| 52/52 [00:00<00:00, 52.02it/s]\n",
      "Link 223/284, Test: 100%|████████████████████████████| 277/277 [00:01<00:00, 214.59it/s]\n",
      "Link 224/284, Test: 100%|████████████████████████████| 172/172 [00:01<00:00, 146.20it/s]\n",
      "Link 225/284, Test: 100%|████████████████████████████| 268/268 [00:01<00:00, 206.71it/s]\n",
      "Link 226/284, Test: 100%|████████████████████████████| 260/260 [00:01<00:00, 201.17it/s]\n",
      "Link 227/284, Test: 100%|███████████████████████████████| 46/46 [00:01<00:00, 37.83it/s]\n",
      "Link 228/284, Test: 100%|████████████████████████████| 253/253 [00:01<00:00, 165.98it/s]\n",
      "Link 229/284, Test: 100%|███████████████████████████████| 78/78 [00:01<00:00, 75.03it/s]\n",
      "Link 230/284, Test: 100%|████████████████████████████| 272/272 [00:01<00:00, 207.70it/s]\n",
      "Link 231/284, Test: 100%|████████████████████████████| 223/223 [00:01<00:00, 183.13it/s]\n",
      "Link 232/284, Test: 100%|████████████████████████████| 247/247 [00:01<00:00, 192.38it/s]\n",
      "Link 233/284, Test: 100%|████████████████████████████| 251/251 [00:01<00:00, 196.35it/s]\n",
      "Link 234/284, Test: 100%|████████████████████████████| 212/212 [00:01<00:00, 172.39it/s]\n",
      "Link 235/284, Test: 100%|████████████████████████████| 212/212 [00:01<00:00, 173.66it/s]\n",
      "Link 236/284, Test: 100%|███████████████████████████████| 67/67 [00:01<00:00, 65.68it/s]\n",
      "Link 237/284, Test: 100%|████████████████████████████| 262/262 [00:01<00:00, 202.59it/s]\n",
      "Link 238/284, Test: 100%|████████████████████████████| 196/196 [00:01<00:00, 162.50it/s]\n",
      "Link 239/284, Test: 100%|████████████████████████████| 251/251 [00:01<00:00, 193.74it/s]\n",
      "Link 240/284, Test: 100%|████████████████████████████| 160/160 [00:01<00:00, 137.34it/s]\n",
      "Link 241/284, Test: 100%|█████████████████████████████| 109/109 [00:01<00:00, 97.91it/s]\n",
      "Link 242/284, Test: 100%|███████████████████████████████| 98/98 [00:01<00:00, 91.54it/s]\n",
      "Link 243/284, Test: 100%|████████████████████████████| 227/227 [00:01<00:00, 182.11it/s]\n",
      "Link 244/284, Test: 100%|████████████████████████████| 240/240 [00:01<00:00, 193.98it/s]\n",
      "Link 245/284, Test: 100%|████████████████████████████| 257/257 [00:01<00:00, 200.67it/s]\n",
      "Link 246/284, Test: 100%|████████████████████████████| 140/140 [00:01<00:00, 126.48it/s]\n",
      "Link 247/284, Test: 100%|███████████████████████████████| 46/46 [00:01<00:00, 45.14it/s]\n",
      "Link 248/284, Test: 100%|███████████████████████████████| 91/91 [00:01<00:00, 88.36it/s]\n",
      "Link 249/284, Test: 100%|████████████████████████████| 274/274 [00:01<00:00, 207.98it/s]\n",
      "Link 250/284, Test: 100%|████████████████████████████| 244/244 [00:01<00:00, 194.60it/s]\n",
      "Link 251/284, Test: 100%|████████████████████████████| 255/255 [00:01<00:00, 201.86it/s]\n",
      "Link 252/284, Test: 100%|████████████████████████████| 163/163 [00:01<00:00, 140.10it/s]\n",
      "Link 253/284, Test: 100%|████████████████████████████| 242/242 [00:01<00:00, 188.96it/s]\n",
      "Link 254/284, Test: 100%|███████████████████████████████| 72/72 [00:01<00:00, 70.19it/s]\n",
      "Link 255/284, Test: 100%|████████████████████████████| 259/259 [00:01<00:00, 201.18it/s]\n",
      "Link 256/284, Test: 100%|████████████████████████████| 218/218 [00:01<00:00, 179.84it/s]\n",
      "Link 257/284, Test: 100%|████████████████████████████| 224/224 [00:01<00:00, 182.85it/s]\n",
      "Link 258/284, Test: 100%|████████████████████████████| 194/194 [00:01<00:00, 164.12it/s]\n",
      "Link 259/284, Test: 100%|███████████████████████████████| 62/62 [00:01<00:00, 59.81it/s]\n",
      "Link 260/284, Test: 100%|████████████████████████████| 154/154 [00:01<00:00, 135.50it/s]\n",
      "Link 261/284, Test: 100%|███████████████████████████████| 72/72 [00:01<00:00, 70.40it/s]\n",
      "Link 262/284, Test: 100%|███████████████████████████████| 88/88 [00:01<00:00, 85.06it/s]\n",
      "Link 263/284, Test: 100%|████████████████████████████| 231/231 [00:01<00:00, 182.96it/s]\n",
      "Link 264/284, Test: 100%|████████████████████████████| 197/197 [00:01<00:00, 160.66it/s]\n",
      "Link 265/284, Test: 100%|████████████████████████████| 128/128 [00:01<00:00, 116.55it/s]\n",
      "Link 266/284, Test: 100%|████████████████████████████| 250/250 [00:01<00:00, 193.13it/s]\n",
      "Link 267/284, Test: 100%|███████████████████████████████| 50/50 [00:00<00:00, 50.68it/s]\n",
      "Link 268/284, Test: 100%|████████████████████████████| 238/238 [00:01<00:00, 190.97it/s]\n",
      "Link 269/284, Test: 100%|███████████████████████████████| 46/46 [00:00<00:00, 46.86it/s]\n",
      "Link 270/284, Test: 100%|████████████████████████████| 272/272 [00:01<00:00, 199.41it/s]\n",
      "Link 271/284, Test: 100%|████████████████████████████| 136/136 [00:01<00:00, 124.14it/s]\n",
      "Link 272/284, Test: 100%|████████████████████████████| 252/252 [00:01<00:00, 193.16it/s]\n",
      "Link 273/284, Test: 100%|████████████████████████████| 233/233 [00:01<00:00, 177.57it/s]\n",
      "Link 274/284, Test: 100%|████████████████████████████| 259/259 [00:01<00:00, 198.24it/s]\n",
      "Link 275/284, Test: 100%|████████████████████████████| 235/235 [00:01<00:00, 188.20it/s]\n",
      "Link 276/284, Test: 100%|████████████████████████████| 252/252 [00:01<00:00, 194.41it/s]\n",
      "Link 277/284, Test: 100%|████████████████████████████| 247/247 [00:01<00:00, 188.08it/s]\n",
      "Link 278/284, Test: 100%|████████████████████████████| 256/256 [00:01<00:00, 189.84it/s]\n",
      "Link 279/284, Test: 100%|████████████████████████████| 242/242 [00:01<00:00, 154.12it/s]\n",
      "Link 280/284, Test: 100%|████████████████████████████| 196/196 [00:01<00:00, 160.23it/s]\n",
      "Link 281/284, Test: 100%|████████████████████████████| 227/227 [00:01<00:00, 182.70it/s]\n",
      "Link 282/284, Test: 100%|████████████████████████████| 194/194 [00:01<00:00, 133.29it/s]\n",
      "Link 283/284, Test: 100%|████████████████████████████| 160/160 [00:01<00:00, 137.60it/s]\n",
      "Link 284/284, Test: 100%|████████████████████████████| 247/247 [00:01<00:00, 192.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test Accuracy per Link\n",
    "\n",
    "accr = []\n",
    "\n",
    "\n",
    "for i in range(total_links - test_start):\n",
    "    test_data_i = [link_data[i+test_start]]\n",
    "    test_x_i = torch.cat(tuple(link[0] for link in test_data_i),dim=0)\n",
    "    test_y_i = torch.cat(tuple(link[1] for link in test_data_i),dim=0)\n",
    "    test_x_i = test_x_i.view(-1,1,2)\n",
    "    \n",
    "    test_dataloader = data.DataLoader(\n",
    "    data.TensorDataset(test_x_i,test_y_i), \n",
    "    batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "    progress_test = tqdm(\n",
    "        test_dataloader, \n",
    "        desc=f'Link {i+test_start+1}/{total_links}, Test',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(test_dataloader), smoothing=.9)\n",
    "\n",
    "    predict = []\n",
    "    target = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (batch_input, batch_target) in enumerate(progress_test):\n",
    "            batch_input = batch_input.cuda()\n",
    "            batch_target = batch_target.cuda()\n",
    "            batch_predict = model(batch_input)\n",
    "            predict.append(batch_predict.argmax(dim=1).cpu().numpy())\n",
    "            target.append(batch_target.cpu().numpy())        \n",
    "    predict = np.concatenate(predict, axis=0)\n",
    "    target = np.concatenate(target, axis=0)\n",
    "\n",
    "    tp = predict[target==1].sum()\n",
    "    tn = (target==0).sum() - predict[target==0].sum()\n",
    "    fp = predict[target==0].sum()\n",
    "    fn = (target==1).sum() - predict[target==1].sum()\n",
    "    accr.append((tp+tn)/(tp+tn+fp+fn))    \n",
    "\n",
    "cbsdnn_scr4_res['cbsdnn_scr4_accr_perlink'] = accr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Pilot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training: 100%|████████████████████████| 2548/2548 [00:07<00:00, 323.83it/s]\n",
      "Epoch 1/10, Validation: 100%|██████████████████████| 5096/5096 [00:13<00:00, 386.55it/s]\n",
      "Epoch 2/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.54985, validation loss: 0.66600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Training: 100%|████████████████████████| 2548/2548 [00:06<00:00, 385.43it/s]\n",
      "Epoch 2/10, Validation: 100%|██████████████████████| 5096/5096 [00:11<00:00, 430.74it/s]\n",
      "Epoch 3/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.53042, validation loss: 0.60135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Training: 100%|████████████████████████| 2548/2548 [00:06<00:00, 392.21it/s]\n",
      "Epoch 3/10, Validation: 100%|██████████████████████| 5096/5096 [00:11<00:00, 429.86it/s]\n",
      "Epoch 4/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.52963, validation loss: 0.60159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Training: 100%|████████████████████████| 2548/2548 [00:06<00:00, 391.33it/s]\n",
      "Epoch 4/10, Validation: 100%|██████████████████████| 5096/5096 [00:11<00:00, 429.94it/s]\n",
      "Epoch 5/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.52920, validation loss: 0.59807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Training: 100%|████████████████████████| 2548/2548 [00:06<00:00, 376.24it/s]\n",
      "Epoch 5/10, Validation: 100%|██████████████████████| 5096/5096 [00:12<00:00, 421.59it/s]\n",
      "Epoch 6/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.52892, validation loss: 0.59760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Training: 100%|████████████████████████| 2548/2548 [00:06<00:00, 400.26it/s]\n",
      "Epoch 6/10, Validation: 100%|██████████████████████| 5096/5096 [00:11<00:00, 436.76it/s]\n",
      "Epoch 7/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.52875, validation loss: 0.59701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Training: 100%|████████████████████████| 2548/2548 [00:06<00:00, 400.95it/s]\n",
      "Epoch 7/10, Validation: 100%|██████████████████████| 5096/5096 [00:11<00:00, 442.12it/s]\n",
      "Epoch 8/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.52863, validation loss: 0.59793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Training: 100%|████████████████████████| 2548/2548 [00:06<00:00, 394.90it/s]\n",
      "Epoch 8/10, Validation: 100%|██████████████████████| 5096/5096 [00:11<00:00, 436.49it/s]\n",
      "Epoch 9/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.52855, validation loss: 0.59623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Training: 100%|████████████████████████| 2548/2548 [00:06<00:00, 392.63it/s]\n",
      "Epoch 9/10, Validation: 100%|██████████████████████| 5096/5096 [00:12<00:00, 418.04it/s]\n",
      "Epoch 10/10, Validation:   0%|                                 | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.52848, validation loss: 0.59764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Training: 100%|███████████████████████| 2548/2548 [00:06<00:00, 401.59it/s]\n",
      "Epoch 10/10, Validation: 100%|█████████████████████| 5096/5096 [00:11<00:00, 445.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.52842, validation loss: 0.59580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "model = MLP().cuda()\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor([1.0,1.0])).cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "train_dataloader=data.DataLoader(data.TensorDataset(train_x_plt,train_y_plt),batch_size=TRAIN_BATCH_SIZE, \n",
    "                                   shuffle=True, num_workers=16, pin_memory=True)\n",
    "\n",
    "val_dataloader=data.DataLoader(data.TensorDataset(val_x_plt,val_y_plt),batch_size=VAL_BATCH_SIZE, \n",
    "                                 shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "test_dataloader=data.DataLoader(data.TensorDataset(test_x_plt,test_y_plt), batch_size=VAL_BATCH_SIZE,\n",
    "                                  shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch_idx in range(NUM_EPOCHS):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "\n",
    "    progress_training_epoch = tqdm(\n",
    "        train_dataloader, \n",
    "        desc=f'Epoch {epoch_idx + 1}/{NUM_EPOCHS}, Training',\n",
    "        miniters=1, ncols=88, position=0,\n",
    "        leave=True, total=len(train_dataloader), smoothing=.9)\n",
    "\n",
    "    progress_validation_epoch = tqdm(\n",
    "        val_dataloader, \n",
    "        desc=f'Epoch {epoch_idx + 1}/{NUM_EPOCHS}, Validation',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(val_dataloader), smoothing=.9)\n",
    "\n",
    "    train_loss = 0\n",
    "    train_size = 0\n",
    "    for idx, (input, target) in enumerate(progress_training_epoch):\n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "        model.zero_grad()\n",
    "        predict = model(input)\n",
    "        loss = loss_function(predict, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss * target.size()[0]\n",
    "        train_size += target.size()[0]\n",
    "\n",
    "    test_loss = 0\n",
    "    test_size = 0\n",
    "    predict = []\n",
    "    target = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (batch_input, batch_target) in enumerate(progress_validation_epoch):\n",
    "            batch_input = batch_input.cuda()\n",
    "            batch_target = batch_target.cuda()\n",
    "            batch_predict = model(batch_input)\n",
    "            loss = loss_function(batch_predict, batch_target)\n",
    "            predict.append(batch_predict.argmax(dim=1).cpu().numpy())\n",
    "            target.append(batch_target.cpu().numpy())        \n",
    "            test_loss += loss * batch_target.size()[0]\n",
    "            test_size += batch_target.size()[0]\n",
    "    predict = np.concatenate(predict, axis=0)\n",
    "    target = np.concatenate(target, axis=0)\n",
    "   \n",
    "    print(f'train loss:{train_loss.item()/train_size: .5f}, '\n",
    "          f'validation loss:{test_loss.item()/test_size: .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test: 100%|████████████████████████| 25479/25479 [00:20<00:00, 1243.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "progress_test_epoch = tqdm(\n",
    "    test_dataloader, \n",
    "    desc=f'Epoch {epoch_idx + 1}/{NUM_EPOCHS}, Test',\n",
    "    miniters=1, ncols=88, position=0, \n",
    "    leave=True, total=len(test_dataloader), smoothing=.9)\n",
    "\n",
    "predict = [] # Predicted Label\n",
    "target = [] # Target Lable\n",
    "snrss = [] # SNR (Calculated from Noise Variance)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (batch_input, batch_target) in enumerate(progress_test_epoch):\n",
    "        batch_input = batch_input.cuda()\n",
    "        batch_target = batch_target.cuda()\n",
    "        batch_predict = model(batch_input)\n",
    "        predict.append(batch_predict.argmax(dim=1).cpu().numpy())\n",
    "        target.append(batch_target.cpu().numpy())        \n",
    "        snrss.append(batch_input.cpu().numpy()[:,0])\n",
    "predict = np.concatenate(predict, axis=0)\n",
    "target = np.concatenate(target, axis=0)\n",
    "snrss = np.concatenate(snrss, axis=0)\n",
    "\n",
    "mlp_scr4_res['mlp_scr4_plt_prd'] = predict\n",
    "mlp_scr4_res['mlp_scr4_plt_trg'] = target\n",
    "mlp_scr4_res['mlp_scr4_plt_snr'] = snrss * scr4_std + scr4_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training: 100%|█████████████████████████| 2548/2548 [01:03<00:00, 40.01it/s]\n",
      "Epoch 1/10, Validation: 100%|███████████████████████| 5096/5096 [01:16<00:00, 66.89it/s]\n",
      "Epoch 2/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.52005, validation loss: 0.60013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Training: 100%|█████████████████████████| 2548/2548 [01:04<00:00, 39.34it/s]\n",
      "Epoch 2/10, Validation: 100%|███████████████████████| 5096/5096 [01:16<00:00, 66.73it/s]\n",
      "Epoch 3/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.50405, validation loss: 0.58992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Training: 100%|█████████████████████████| 2548/2548 [01:02<00:00, 40.86it/s]\n",
      "Epoch 3/10, Validation: 100%|███████████████████████| 5096/5096 [01:14<00:00, 68.56it/s]\n",
      "Epoch 4/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.50012, validation loss: 0.58134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Training: 100%|█████████████████████████| 2548/2548 [01:03<00:00, 40.05it/s]\n",
      "Epoch 4/10, Validation: 100%|███████████████████████| 5096/5096 [01:15<00:00, 67.14it/s]\n",
      "Epoch 5/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.49627, validation loss: 0.57478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Training: 100%|█████████████████████████| 2548/2548 [01:03<00:00, 40.23it/s]\n",
      "Epoch 5/10, Validation: 100%|███████████████████████| 5096/5096 [01:14<00:00, 68.17it/s]\n",
      "Epoch 6/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.49322, validation loss: 0.57051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Training: 100%|█████████████████████████| 2548/2548 [01:03<00:00, 40.03it/s]\n",
      "Epoch 6/10, Validation: 100%|███████████████████████| 5096/5096 [01:16<00:00, 66.80it/s]\n",
      "Epoch 7/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.49079, validation loss: 0.56711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Training: 100%|█████████████████████████| 2548/2548 [01:04<00:00, 39.79it/s]\n",
      "Epoch 7/10, Validation: 100%|███████████████████████| 5096/5096 [01:16<00:00, 66.51it/s]\n",
      "Epoch 8/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.48861, validation loss: 0.56408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Training: 100%|█████████████████████████| 2548/2548 [01:05<00:00, 38.79it/s]\n",
      "Epoch 8/10, Validation: 100%|███████████████████████| 5096/5096 [01:18<00:00, 65.14it/s]\n",
      "Epoch 9/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.48659, validation loss: 0.56152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Training: 100%|█████████████████████████| 2548/2548 [01:05<00:00, 38.73it/s]\n",
      "Epoch 9/10, Validation: 100%|███████████████████████| 5096/5096 [01:18<00:00, 65.18it/s]\n",
      "Epoch 10/10, Validation:   0%|                                 | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.48498, validation loss: 0.55953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Training: 100%|████████████████████████| 2548/2548 [01:04<00:00, 39.69it/s]\n",
      "Epoch 10/10, Validation: 100%|██████████████████████| 5096/5096 [01:16<00:00, 66.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.48379, validation loss: 0.55768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "model = GRUTagger(EMBEDDING_DIM, HIDDEN_DIM, TAGSET_SIZE).cuda()\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor([1.0,1.0])).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), 0.0001)\n",
    "\n",
    "train_dataloader=data.DataLoader(data.TensorDataset(train_x_plt,train_y_plt),batch_size=TRAIN_BATCH_SIZE, \n",
    "                                   shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "val_dataloader=data.DataLoader(data.TensorDataset(val_x_plt,val_y_plt),batch_size=VAL_BATCH_SIZE, \n",
    "                                 shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "test_dataloader=data.DataLoader(data.TensorDataset(test_x_plt,test_y_plt), batch_size=VAL_BATCH_SIZE,\n",
    "                                  shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "for epoch_idx in range(NUM_EPOCHS): \n",
    "\n",
    "    progress_training_epoch = tqdm(\n",
    "        train_dataloader, \n",
    "        desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Training',\n",
    "        miniters=1, ncols=88, position=0,\n",
    "        leave=True, total=len(train_dataloader), smoothing=.9)\n",
    "\n",
    "    progress_validation_epoch = tqdm(\n",
    "        val_dataloader, \n",
    "        desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Validation',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(val_dataloader), smoothing=.9)\n",
    "\n",
    "    train_loss = 0\n",
    "    train_size = 0\n",
    "    model.train()\n",
    "    for idx, (sentence, tags) in enumerate(progress_training_epoch):\n",
    "        sentence = sentence.cuda()\n",
    "        tags = tags.cuda()\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(sentence)\n",
    "        loss = loss_function(tag_scores, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss * tags.size()[0]\n",
    "        train_size += tags.size()[0]\n",
    "\n",
    "    test_loss = 0\n",
    "    test_size = 0\n",
    "    predict = []\n",
    "    target = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (sentence, tags) in enumerate(progress_validation_epoch):\n",
    "            sentence = sentence.cuda()\n",
    "            tags = tags.cuda()\n",
    "            tag_scores = model(sentence)\n",
    "            loss = loss_function(tag_scores, tags)\n",
    "            predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "            target.append(tags.cpu().numpy())        \n",
    "            test_loss += loss * tags.size()[0]\n",
    "            test_size += tags.size()[0]\n",
    "    predict = np.concatenate(predict, axis=0)\n",
    "    target = np.concatenate(target, axis=0)\n",
    "\n",
    "    print(f'train loss:{train_loss.item()/train_size: .5f}, '\n",
    "          f'validation loss:{test_loss.item()/test_size: .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test: 100%|█████████████████████████| 25479/25479 [00:56<00:00, 454.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "progress_test_epoch = tqdm(\n",
    "    test_dataloader, \n",
    "    desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Test',\n",
    "    miniters=1, ncols=88, position=0, \n",
    "    leave=True, total=len(test_dataloader), smoothing=.9)\n",
    "\n",
    "predict = []\n",
    "target = []\n",
    "snrss = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (sentence, tags) in enumerate(progress_test_epoch):\n",
    "        sentence = sentence.cuda()\n",
    "        tags = tags.cuda()\n",
    "        tag_scores = model(sentence)\n",
    "        predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "        target.append(tags.cpu().numpy())\n",
    "        snrss.append(sentence.cpu().numpy()[:,0])\n",
    "        \n",
    "predict = np.concatenate(predict, axis=0)\n",
    "target = np.concatenate(target, axis=0)\n",
    "snrss = np.concatenate(snrss, axis=0)\n",
    "\n",
    "gru_scr4_res['gru_scr4_plt_prd'] = predict\n",
    "gru_scr4_res['gru_scr4_plt_trg'] = target\n",
    "gru_scr4_res['gru_scr4_plt_snr'] = snrss * scr4_std + scr4_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training: 100%|█████████████████████████| 2548/2548 [01:55<00:00, 22.14it/s]\n",
      "Epoch 1/10, Validation: 100%|███████████████████████| 5096/5096 [02:13<00:00, 38.06it/s]\n",
      "Epoch 2/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.48156, validation loss: 0.54465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Training: 100%|█████████████████████████| 2548/2548 [01:55<00:00, 21.98it/s]\n",
      "Epoch 2/10, Validation: 100%|███████████████████████| 5096/5096 [02:14<00:00, 38.00it/s]\n",
      "Epoch 3/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.40024, validation loss: 0.52120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Training: 100%|█████████████████████████| 2548/2548 [01:56<00:00, 21.88it/s]\n",
      "Epoch 3/10, Validation: 100%|███████████████████████| 5096/5096 [02:14<00:00, 37.87it/s]\n",
      "Epoch 4/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.39118, validation loss: 0.50855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Training: 100%|█████████████████████████| 2548/2548 [01:56<00:00, 21.87it/s]\n",
      "Epoch 4/10, Validation: 100%|███████████████████████| 5096/5096 [02:15<00:00, 37.65it/s]\n",
      "Epoch 5/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.38600, validation loss: 0.49750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Training: 100%|█████████████████████████| 2548/2548 [01:55<00:00, 22.13it/s]\n",
      "Epoch 5/10, Validation: 100%|███████████████████████| 5096/5096 [02:13<00:00, 38.06it/s]\n",
      "Epoch 6/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.38272, validation loss: 0.48895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Training: 100%|█████████████████████████| 2548/2548 [01:54<00:00, 22.31it/s]\n",
      "Epoch 6/10, Validation: 100%|███████████████████████| 5096/5096 [02:12<00:00, 38.40it/s]\n",
      "Epoch 7/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.38074, validation loss: 0.48390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Training: 100%|█████████████████████████| 2548/2548 [01:58<00:00, 21.43it/s]\n",
      "Epoch 7/10, Validation: 100%|███████████████████████| 5096/5096 [02:17<00:00, 36.94it/s]\n",
      "Epoch 8/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.37934, validation loss: 0.48079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Training: 100%|█████████████████████████| 2548/2548 [01:55<00:00, 22.08it/s]\n",
      "Epoch 8/10, Validation: 100%|███████████████████████| 5096/5096 [02:12<00:00, 38.32it/s]\n",
      "Epoch 9/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.37810, validation loss: 0.47863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Training: 100%|█████████████████████████| 2548/2548 [01:54<00:00, 22.28it/s]\n",
      "Epoch 9/10, Validation: 100%|███████████████████████| 5096/5096 [02:12<00:00, 38.60it/s]\n",
      "Epoch 10/10, Validation:   0%|                                 | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.37692, validation loss: 0.47706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Training: 100%|████████████████████████| 2548/2548 [01:55<00:00, 22.08it/s]\n",
      "Epoch 10/10, Validation: 100%|██████████████████████| 5096/5096 [02:13<00:00, 38.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.37584, validation loss: 0.47586\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "model = BGRUTagger(EMBEDDING_DIM, HIDDEN_DIM, TAGSET_SIZE).cuda()\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor([1.0,1.0])).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), 0.0001)\n",
    "\n",
    "train_dataloader=data.DataLoader(data.TensorDataset(train_x_plt,train_y_plt),batch_size=TRAIN_BATCH_SIZE, \n",
    "                                   shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "val_dataloader=data.DataLoader(data.TensorDataset(val_x_plt,val_y_plt),batch_size=VAL_BATCH_SIZE, \n",
    "                                 shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "test_dataloader=data.DataLoader(data.TensorDataset(test_x_plt,test_y_plt), batch_size=VAL_BATCH_SIZE,\n",
    "                                  shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "for epoch_idx in range(NUM_EPOCHS): \n",
    "\n",
    "    progress_training_epoch = tqdm(\n",
    "        train_dataloader, \n",
    "        desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Training',\n",
    "        miniters=1, ncols=88, position=0,\n",
    "        leave=True, total=len(train_dataloader), smoothing=.9)\n",
    "\n",
    "    progress_validation_epoch = tqdm(\n",
    "        val_dataloader, \n",
    "        desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Validation',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(val_dataloader), smoothing=.9)\n",
    "\n",
    "    train_loss = 0\n",
    "    train_size = 0\n",
    "    model.train()\n",
    "    for idx, (sentence, tags) in enumerate(progress_training_epoch):\n",
    "        sentence = sentence.cuda()\n",
    "        tags = tags.cuda()\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(sentence)\n",
    "        loss = loss_function(tag_scores, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss * tags.size()[0]\n",
    "        train_size += tags.size()[0]\n",
    "\n",
    "    test_loss = 0\n",
    "    test_size = 0\n",
    "    predict = []\n",
    "    target = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (sentence, tags) in enumerate(progress_validation_epoch):\n",
    "            sentence = sentence.cuda()\n",
    "            tags = tags.cuda()\n",
    "            tag_scores = model(sentence)\n",
    "            loss = loss_function(tag_scores, tags)\n",
    "            predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "            target.append(tags.cpu().numpy())        \n",
    "            test_loss += loss * tags.size()[0]\n",
    "            test_size += tags.size()[0]\n",
    "    predict = np.concatenate(predict, axis=0)\n",
    "    target = np.concatenate(target, axis=0)\n",
    "\n",
    "    print(f'train loss:{train_loss.item()/train_size: .5f}, '\n",
    "          f'validation loss:{test_loss.item()/test_size: .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test: 100%|█████████████████████████| 25479/25479 [01:27<00:00, 292.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "progress_test_epoch = tqdm(\n",
    "    test_dataloader, \n",
    "    desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Test',\n",
    "    miniters=1, ncols=88, position=0, \n",
    "    leave=True, total=len(test_dataloader), smoothing=.9)\n",
    "\n",
    "predict = []\n",
    "target = []\n",
    "snrss = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (sentence, tags) in enumerate(progress_test_epoch):\n",
    "        sentence = sentence.cuda()\n",
    "        tags = tags.cuda()\n",
    "        tag_scores = model(sentence)\n",
    "        predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "        target.append(tags.cpu().numpy())\n",
    "        snrss.append(sentence.cpu().numpy()[:,0])\n",
    "        \n",
    "predict = np.concatenate(predict, axis=0)\n",
    "target = np.concatenate(target, axis=0)\n",
    "snrss = np.concatenate(snrss, axis=0)\n",
    "\n",
    "bgru_scr4_res['bgru_scr4_plt_prd'] = predict\n",
    "bgru_scr4_res['bgru_scr4_plt_trg'] = target\n",
    "bgru_scr4_res['bgru_scr4_plt_snr'] = snrss * scr4_std + scr4_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BSRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training: 100%|████████████████████████| 2548/2548 [00:20<00:00, 126.93it/s]\n",
      "Epoch 1/10, Validation: 100%|██████████████████████| 5096/5096 [00:26<00:00, 193.66it/s]\n",
      "Epoch 2/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.47413, validation loss: 0.53364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Training: 100%|████████████████████████| 2548/2548 [00:15<00:00, 163.86it/s]\n",
      "Epoch 2/10, Validation: 100%|██████████████████████| 5096/5096 [00:22<00:00, 229.26it/s]\n",
      "Epoch 3/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.41861, validation loss: 0.50588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Training: 100%|████████████████████████| 2548/2548 [00:15<00:00, 160.99it/s]\n",
      "Epoch 3/10, Validation: 100%|██████████████████████| 5096/5096 [00:22<00:00, 227.33it/s]\n",
      "Epoch 4/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.40382, validation loss: 0.49455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Training: 100%|████████████████████████| 2548/2548 [00:15<00:00, 161.48it/s]\n",
      "Epoch 4/10, Validation: 100%|██████████████████████| 5096/5096 [00:22<00:00, 230.69it/s]\n",
      "Epoch 5/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.39673, validation loss: 0.48787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Training: 100%|████████████████████████| 2548/2548 [00:15<00:00, 163.69it/s]\n",
      "Epoch 5/10, Validation: 100%|██████████████████████| 5096/5096 [00:22<00:00, 229.37it/s]\n",
      "Epoch 6/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.39252, validation loss: 0.48359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Training: 100%|████████████████████████| 2548/2548 [00:15<00:00, 164.71it/s]\n",
      "Epoch 6/10, Validation: 100%|██████████████████████| 5096/5096 [00:21<00:00, 231.71it/s]\n",
      "Epoch 7/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.38925, validation loss: 0.47947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Training: 100%|████████████████████████| 2548/2548 [00:15<00:00, 162.51it/s]\n",
      "Epoch 7/10, Validation: 100%|██████████████████████| 5096/5096 [00:22<00:00, 229.52it/s]\n",
      "Epoch 8/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.38657, validation loss: 0.47675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Training: 100%|████████████████████████| 2548/2548 [00:15<00:00, 162.22it/s]\n",
      "Epoch 8/10, Validation: 100%|██████████████████████| 5096/5096 [00:22<00:00, 227.09it/s]\n",
      "Epoch 9/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.38453, validation loss: 0.47428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Training: 100%|████████████████████████| 2548/2548 [00:15<00:00, 159.54it/s]\n",
      "Epoch 9/10, Validation: 100%|██████████████████████| 5096/5096 [00:22<00:00, 225.72it/s]\n",
      "Epoch 10/10, Validation:   0%|                                 | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.38282, validation loss: 0.47242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Training: 100%|███████████████████████| 2548/2548 [00:15<00:00, 160.79it/s]\n",
      "Epoch 10/10, Validation: 100%|█████████████████████| 5096/5096 [00:22<00:00, 222.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.38138, validation loss: 0.47104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "model = BSRUTagger(EMBEDDING_DIM, HIDDEN_DIM, TAGSET_SIZE).cuda()\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor([1.0,1.0])).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), 0.0001)\n",
    "\n",
    "train_dataloader=data.DataLoader(data.TensorDataset(train_x_plt,train_y_plt),batch_size=TRAIN_BATCH_SIZE, \n",
    "                                   shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "val_dataloader=data.DataLoader(data.TensorDataset(val_x_plt,val_y_plt),batch_size=VAL_BATCH_SIZE, \n",
    "                                 shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "test_dataloader=data.DataLoader(data.TensorDataset(test_x_plt,test_y_plt), batch_size=VAL_BATCH_SIZE,\n",
    "                                  shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "for epoch_idx in range(NUM_EPOCHS): \n",
    "\n",
    "    progress_training_epoch = tqdm(\n",
    "        train_dataloader, \n",
    "        desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Training',\n",
    "        miniters=1, ncols=88, position=0,\n",
    "        leave=True, total=len(train_dataloader), smoothing=.9)\n",
    "\n",
    "    progress_validation_epoch = tqdm(\n",
    "        val_dataloader, \n",
    "        desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Validation',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(val_dataloader), smoothing=.9)\n",
    "\n",
    "    train_loss = 0\n",
    "    train_size = 0\n",
    "    model.train()\n",
    "    for idx, (sentence, tags) in enumerate(progress_training_epoch):\n",
    "        sentence = sentence.cuda()\n",
    "        tags = tags.cuda()\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(sentence)\n",
    "        loss = loss_function(tag_scores, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss * tags.size()[0]\n",
    "        train_size += tags.size()[0]\n",
    "\n",
    "    test_loss = 0\n",
    "    test_size = 0\n",
    "    predict = []\n",
    "    target = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (sentence, tags) in enumerate(progress_validation_epoch):\n",
    "            sentence = sentence.cuda()\n",
    "            tags = tags.cuda()\n",
    "            tag_scores = model(sentence)\n",
    "            loss = loss_function(tag_scores, tags)\n",
    "            predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "            target.append(tags.cpu().numpy())        \n",
    "            test_loss += loss * tags.size()[0]\n",
    "            test_size += tags.size()[0]\n",
    "    predict = np.concatenate(predict, axis=0)\n",
    "    target = np.concatenate(target, axis=0)\n",
    "\n",
    "    print(f'train loss:{train_loss.item()/train_size: .5f}, '\n",
    "          f'validation loss:{test_loss.item()/test_size: .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test: 100%|█████████████████████████| 25479/25479 [00:30<00:00, 827.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "progress_test_epoch = tqdm(\n",
    "    test_dataloader, \n",
    "    desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Test',\n",
    "    miniters=1, ncols=88, position=0, \n",
    "    leave=True, total=len(test_dataloader), smoothing=.9)\n",
    "\n",
    "predict = []\n",
    "target = []\n",
    "snrss = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (sentence, tags) in enumerate(progress_test_epoch):\n",
    "        sentence = sentence.cuda()\n",
    "        tags = tags.cuda()\n",
    "        tag_scores = model(sentence)\n",
    "        predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "        target.append(tags.cpu().numpy())\n",
    "        snrss.append(sentence.cpu().numpy()[:,0])\n",
    "        \n",
    "predict = np.concatenate(predict, axis=0)\n",
    "target = np.concatenate(target, axis=0)\n",
    "snrss = np.concatenate(snrss, axis=0)\n",
    "\n",
    "bsru_scr4_res['bsru_scr4_plt_prd'] = predict\n",
    "bsru_scr4_res['bsru_scr4_plt_trg'] = target\n",
    "bsru_scr4_res['bsru_scr4_plt_snr'] = snrss * scr4_std + scr4_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBSDNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/10, Training: 100%|████████████████████████| 2548/2548 [00:20<00:00, 123.00it/s]\n",
      "Epoch 0/10, Validation: 100%|██████████████████████| 5096/5096 [00:29<00:00, 172.90it/s]\n",
      "Epoch 1/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.47954, validation loss: 0.59755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training: 100%|████████████████████████| 2548/2548 [00:21<00:00, 121.27it/s]\n",
      "Epoch 1/10, Validation: 100%|██████████████████████| 5096/5096 [00:30<00:00, 168.54it/s]\n",
      "Epoch 2/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.41107, validation loss: 0.54286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Training: 100%|████████████████████████| 2548/2548 [00:20<00:00, 123.05it/s]\n",
      "Epoch 2/10, Validation: 100%|██████████████████████| 5096/5096 [00:29<00:00, 173.73it/s]\n",
      "Epoch 3/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.39956, validation loss: 0.52258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Training: 100%|████████████████████████| 2548/2548 [00:20<00:00, 122.52it/s]\n",
      "Epoch 3/10, Validation: 100%|██████████████████████| 5096/5096 [00:29<00:00, 172.86it/s]\n",
      "Epoch 4/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.39419, validation loss: 0.51233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Training: 100%|████████████████████████| 2548/2548 [00:20<00:00, 124.15it/s]\n",
      "Epoch 4/10, Validation: 100%|██████████████████████| 5096/5096 [00:29<00:00, 174.30it/s]\n",
      "Epoch 5/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.39090, validation loss: 0.50504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Training: 100%|████████████████████████| 2548/2548 [00:20<00:00, 123.31it/s]\n",
      "Epoch 5/10, Validation: 100%|██████████████████████| 5096/5096 [00:29<00:00, 174.33it/s]\n",
      "Epoch 6/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.38790, validation loss: 0.49648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Training: 100%|████████████████████████| 2548/2548 [00:20<00:00, 123.79it/s]\n",
      "Epoch 6/10, Validation: 100%|██████████████████████| 5096/5096 [00:29<00:00, 175.29it/s]\n",
      "Epoch 7/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.38538, validation loss: 0.49277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Training: 100%|████████████████████████| 2548/2548 [00:20<00:00, 123.07it/s]\n",
      "Epoch 7/10, Validation: 100%|██████████████████████| 5096/5096 [00:29<00:00, 175.37it/s]\n",
      "Epoch 8/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.38365, validation loss: 0.48927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Training: 100%|████████████████████████| 2548/2548 [00:20<00:00, 124.14it/s]\n",
      "Epoch 8/10, Validation: 100%|██████████████████████| 5096/5096 [00:28<00:00, 175.98it/s]\n",
      "Epoch 9/10, Validation:   0%|                                  | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.38216, validation loss: 0.48565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Training: 100%|████████████████████████| 2548/2548 [00:20<00:00, 125.36it/s]\n",
      "Epoch 9/10, Validation: 100%|██████████████████████| 5096/5096 [00:28<00:00, 179.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.38102, validation loss: 0.48474\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model = CBSDNN().cuda()\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor([1.0,1.0])).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "train_dataloader=data.DataLoader(data.TensorDataset(train_x_plt_cbsdnn,train_y_plt),batch_size=TRAIN_BATCH_SIZE, \n",
    "                                   shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "val_dataloader=data.DataLoader(data.TensorDataset(val_x_plt_cbsdnn,val_y_plt),batch_size=VAL_BATCH_SIZE, \n",
    "                                 shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "test_dataloader=data.DataLoader(data.TensorDataset(test_x_plt_cbsdnn,test_y_plt), batch_size=VAL_BATCH_SIZE,\n",
    "                                  shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch_idx in range(NUM_EPOCHS):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "\n",
    "    progress_training_epoch = tqdm(\n",
    "        train_dataloader, \n",
    "        desc=f'Epoch {epoch_idx}/{NUM_EPOCHS}, Training',\n",
    "        miniters=1, ncols=88, position=0,\n",
    "        leave=True, total=len(train_dataloader), smoothing=.9)\n",
    "\n",
    "    progress_validation_epoch = tqdm(\n",
    "        val_dataloader, \n",
    "        desc=f'Epoch {epoch_idx}/{NUM_EPOCHS}, Validation',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(val_dataloader), smoothing=.9)\n",
    "\n",
    "    train_loss = 0\n",
    "    train_size = 0\n",
    "    model.train()\n",
    "\n",
    "    for idx, (input, target) in enumerate(progress_training_epoch):\n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "        model.zero_grad()\n",
    "        predict = model(input)\n",
    "        loss = loss_function(predict, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss * target.size()[0]\n",
    "        train_size += target.size()[0]\n",
    "\n",
    "    test_loss = 0\n",
    "    test_size = 0\n",
    "    predict = []\n",
    "    target = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (batch_input, batch_target) in enumerate(progress_validation_epoch):\n",
    "            batch_input = batch_input.cuda()\n",
    "            batch_target = batch_target.cuda()\n",
    "            batch_predict = model(batch_input)\n",
    "            loss = loss_function(batch_predict, batch_target)\n",
    "            predict.append(batch_predict.argmax(dim=1).cpu().numpy())\n",
    "            target.append(batch_target.cpu().numpy())        \n",
    "            test_loss += loss * batch_target.size()[0]\n",
    "            test_size += batch_target.size()[0]\n",
    "    predict = np.concatenate(predict, axis=0)\n",
    "    target = np.concatenate(target, axis=0)\n",
    "  \n",
    "    print(f'train loss:{train_loss.item()/train_size: .5f}, '\n",
    "          f'validation loss:{test_loss.item()/test_size: .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Test: 100%|█████████████████████████| 25479/25479 [00:38<00:00, 653.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "progress_test_epoch = tqdm(\n",
    "    test_dataloader, \n",
    "    desc=f'Epoch {epoch_idx+1}/{NUM_EPOCHS}, Test',\n",
    "    miniters=1, ncols=88, position=0, \n",
    "    leave=True, total=len(test_dataloader), smoothing=.9)\n",
    "\n",
    "predict = []\n",
    "target = []\n",
    "snrss = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (batch_input, batch_target) in enumerate(progress_test_epoch):\n",
    "        batch_input = batch_input.cuda()\n",
    "        batch_target = batch_target.cuda()\n",
    "        batch_predict = model(batch_input)\n",
    "        predict.append(batch_predict.argmax(dim=1).cpu().numpy())\n",
    "        target.append(batch_target.cpu().numpy())        \n",
    "        snrss.append(batch_input.cpu().numpy()[:,0][:,0])\n",
    "predict = np.concatenate(predict, axis=0)\n",
    "target = np.concatenate(target, axis=0)\n",
    "snrss = np.concatenate(snrss, axis=0)\n",
    "\n",
    "cbsdnn_scr4_res['cbsdnn_scr4_plt_prd'] = predict\n",
    "cbsdnn_scr4_res['cbsdnn_scr4_plt_trg'] = target\n",
    "cbsdnn_scr4_res['cbsdnn_scr4_plt_snr'] = snrss * scr4_std + scr4_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "outfile = os.path.join(os.getcwd(), 'mlp_scr4_res.pickle')\n",
    "if os.path.exists(outfile):\n",
    "        os.replace(outfile, outfile + \".old\")\n",
    "        \n",
    "with open(outfile, 'wb') as file:\n",
    "    pickle.dump(mlp_scr4_res, file)\n",
    "    \n",
    "# GRU\n",
    "outfile = os.path.join(os.getcwd(), 'gru_scr4_res.pickle')\n",
    "if os.path.exists(outfile):\n",
    "        os.replace(outfile, outfile + \".old\")\n",
    "        \n",
    "with open(outfile, 'wb') as file:\n",
    "    pickle.dump(gru_scr4_res, file)\n",
    "    \n",
    "# BGRU\n",
    "outfile = os.path.join(os.getcwd(), 'bgru_scr4_res.pickle')\n",
    "if os.path.exists(outfile):\n",
    "        os.replace(outfile, outfile + \".old\")\n",
    "        \n",
    "with open(outfile, 'wb') as file:\n",
    "    pickle.dump(bgru_scr4_res, file)\n",
    "    \n",
    "# BSRU\n",
    "outfile = os.path.join(os.getcwd(), 'bsru_scr4_res.pickle')\n",
    "if os.path.exists(outfile):\n",
    "        os.replace(outfile, outfile + \".old\")\n",
    "        \n",
    "with open(outfile, 'wb') as file:\n",
    "    pickle.dump(bsru_scr4_res, file)\n",
    "\n",
    "# CBSDNN\n",
    "outfile = os.path.join(os.getcwd(), 'cbsdnn_scr4_res.pickle')\n",
    "if os.path.exists(outfile):\n",
    "        os.replace(outfile, outfile + \".old\")\n",
    "        \n",
    "with open(outfile, 'wb') as file:\n",
    "    pickle.dump(cbsdnn_scr4_res, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
